{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Transfer_Learning_CIFAR10_LinkedIn_Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNz5W7feQ/v33KVa0iLr5D/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a09d9dfdcb554aa5a9870c6770449267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_17b57f58ca9c44c7a229ec43d0d811ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4edb3d442d6d4d43935db0ee8e6bc913",
              "IPY_MODEL_d7203fd5059248e8bf757231169c3aaa"
            ]
          }
        },
        "17b57f58ca9c44c7a229ec43d0d811ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4edb3d442d6d4d43935db0ee8e6bc913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a898d28c0a547f48630e933565153cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_156845c1c9254b70a9b078219395c19a"
          }
        },
        "d7203fd5059248e8bf757231169c3aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8efad0ce99a94284a3bc6091bf2ae4ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 97.8M/97.8M [00:01&lt;00:00, 90.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_946c0674a536456c92f7da15362a9b92"
          }
        },
        "9a898d28c0a547f48630e933565153cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "156845c1c9254b70a9b078219395c19a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8efad0ce99a94284a3bc6091bf2ae4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "946c0674a536456c92f7da15362a9b92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anbansal/PyTorch-Linkedin-Training/blob/master/PyTorch_Transfer_Learning_CIFAR10_LinkedIn_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr4wiezqTkZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmark=False\n",
        "torch.manual_seed(0)\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ekufuytVZ0s",
        "colab_type": "code",
        "outputId": "9ab06f45-6a1f-40f8-8809-be9403a1e14a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08P3SN9mVrR3",
        "colab_type": "code",
        "outputId": "343d0f64-0195-4894-ac8a-6e3e395e2c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "transforms = transforms.Compose([\n",
        "                                 transforms.Resize((224,224)),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize(mean=mean,std=std)\n",
        "                                 ])\n",
        "trainset=datasets.CIFAR10('~/.pytorch/CIFAR/',train=True,transform=transforms,download=True)\n",
        "testset=datasets.CIFAR10('~/.pytorch/CIFAR/',train=False,transform=transforms,download=True)\n",
        "\n",
        "trainloader=DataLoader(trainset,batch_size=64,shuffle=True)\n",
        "testloader=DataLoader(testset,batch_size=64,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmVMtWFNgVLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denormalize(tensor):\n",
        "  tensor = tensor*std+ mean\n",
        "  return tensor\n",
        "\n",
        "def show_img(img):\n",
        "  img = img.numpy().transpose((1,2,0))\n",
        "  img = denormalize(img)\n",
        "  img = np.clip(img,0,1)\n",
        "  plt.imshow(img)\n",
        "  \n",
        "def get_CIFAR10_class(id):\n",
        "  CIFAR10_classes = ['plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "  return CIFAR10_classes[id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nue0DqwW3xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.vgg16(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co-kydxjX5Vj",
        "colab_type": "code",
        "outputId": "dd0af189-d079-4bd8-a54c-a997b57764f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5aWYUOSZo0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for params in model.parameters():\n",
        "  params.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61S7n_COZZBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.classifier[-1] = nn.Sequential(\n",
        "                                      nn.Linear(in_features=4096, out_features=10),\n",
        "                                      nn.LogSoftmax(dim=1)\n",
        "                                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck37GGYaaY6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh7LsojSao7r",
        "colab_type": "code",
        "outputId": "c78c75c5-2183-4418-f675-de4fb10266bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "num_epochs = 1\n",
        "batch_loss = 0\n",
        "cum_loss = 0\n",
        "for e in range(num_epochs):\n",
        "  for batch, (images,labels) in enumerate(trainloader,1):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logps=model(images)\n",
        "    loss=criterion(logps,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_loss += loss.item()\n",
        "    print(f'Epoch {e}/{num_epochs}: Batch {batch}/{len(trainloader)}: Batch Loss: {loss.item()}')\n",
        "print(f'Training Loss: {batch_loss/len(trainloader)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Batch 1/782: Batch Loss: 2.330793857574463\n",
            "Epoch 0/1: Batch 2/782: Batch Loss: 2.1873044967651367\n",
            "Epoch 0/1: Batch 3/782: Batch Loss: 2.0241212844848633\n",
            "Epoch 0/1: Batch 4/782: Batch Loss: 2.1179025173187256\n",
            "Epoch 0/1: Batch 5/782: Batch Loss: 1.783939003944397\n",
            "Epoch 0/1: Batch 6/782: Batch Loss: 1.7197850942611694\n",
            "Epoch 0/1: Batch 7/782: Batch Loss: 1.4781049489974976\n",
            "Epoch 0/1: Batch 8/782: Batch Loss: 1.509700894355774\n",
            "Epoch 0/1: Batch 9/782: Batch Loss: 1.373780369758606\n",
            "Epoch 0/1: Batch 10/782: Batch Loss: 1.2858160734176636\n",
            "Epoch 0/1: Batch 11/782: Batch Loss: 1.196153163909912\n",
            "Epoch 0/1: Batch 12/782: Batch Loss: 1.2627209424972534\n",
            "Epoch 0/1: Batch 13/782: Batch Loss: 1.0433933734893799\n",
            "Epoch 0/1: Batch 14/782: Batch Loss: 1.1577554941177368\n",
            "Epoch 0/1: Batch 15/782: Batch Loss: 1.1435178518295288\n",
            "Epoch 0/1: Batch 16/782: Batch Loss: 1.1732757091522217\n",
            "Epoch 0/1: Batch 17/782: Batch Loss: 0.9265352487564087\n",
            "Epoch 0/1: Batch 18/782: Batch Loss: 1.016086220741272\n",
            "Epoch 0/1: Batch 19/782: Batch Loss: 0.7355153560638428\n",
            "Epoch 0/1: Batch 20/782: Batch Loss: 0.8364719152450562\n",
            "Epoch 0/1: Batch 21/782: Batch Loss: 1.0696858167648315\n",
            "Epoch 0/1: Batch 22/782: Batch Loss: 0.986110270023346\n",
            "Epoch 0/1: Batch 23/782: Batch Loss: 0.6519598364830017\n",
            "Epoch 0/1: Batch 24/782: Batch Loss: 0.751669704914093\n",
            "Epoch 0/1: Batch 25/782: Batch Loss: 0.9761700630187988\n",
            "Epoch 0/1: Batch 26/782: Batch Loss: 0.8610116243362427\n",
            "Epoch 0/1: Batch 27/782: Batch Loss: 0.7925167679786682\n",
            "Epoch 0/1: Batch 28/782: Batch Loss: 0.8034414052963257\n",
            "Epoch 0/1: Batch 29/782: Batch Loss: 0.6840106248855591\n",
            "Epoch 0/1: Batch 30/782: Batch Loss: 0.7157387137413025\n",
            "Epoch 0/1: Batch 31/782: Batch Loss: 1.050793170928955\n",
            "Epoch 0/1: Batch 32/782: Batch Loss: 0.8375429511070251\n",
            "Epoch 0/1: Batch 33/782: Batch Loss: 0.5041617751121521\n",
            "Epoch 0/1: Batch 34/782: Batch Loss: 0.8423767685890198\n",
            "Epoch 0/1: Batch 35/782: Batch Loss: 0.7471191883087158\n",
            "Epoch 0/1: Batch 36/782: Batch Loss: 0.800035834312439\n",
            "Epoch 0/1: Batch 37/782: Batch Loss: 0.5254032611846924\n",
            "Epoch 0/1: Batch 38/782: Batch Loss: 0.6387345790863037\n",
            "Epoch 0/1: Batch 39/782: Batch Loss: 0.7517703175544739\n",
            "Epoch 0/1: Batch 40/782: Batch Loss: 0.6613238453865051\n",
            "Epoch 0/1: Batch 41/782: Batch Loss: 0.9319733381271362\n",
            "Epoch 0/1: Batch 42/782: Batch Loss: 0.6779435873031616\n",
            "Epoch 0/1: Batch 43/782: Batch Loss: 0.7254306077957153\n",
            "Epoch 0/1: Batch 44/782: Batch Loss: 0.7334572672843933\n",
            "Epoch 0/1: Batch 45/782: Batch Loss: 0.7238782644271851\n",
            "Epoch 0/1: Batch 46/782: Batch Loss: 0.8280301690101624\n",
            "Epoch 0/1: Batch 47/782: Batch Loss: 0.7601578831672668\n",
            "Epoch 0/1: Batch 48/782: Batch Loss: 0.9079889059066772\n",
            "Epoch 0/1: Batch 49/782: Batch Loss: 0.8672795295715332\n",
            "Epoch 0/1: Batch 50/782: Batch Loss: 0.6199336647987366\n",
            "Epoch 0/1: Batch 51/782: Batch Loss: 0.8425130844116211\n",
            "Epoch 0/1: Batch 52/782: Batch Loss: 0.6924925446510315\n",
            "Epoch 0/1: Batch 53/782: Batch Loss: 0.9972561597824097\n",
            "Epoch 0/1: Batch 54/782: Batch Loss: 0.8126490712165833\n",
            "Epoch 0/1: Batch 55/782: Batch Loss: 0.9193160533905029\n",
            "Epoch 0/1: Batch 56/782: Batch Loss: 0.511406421661377\n",
            "Epoch 0/1: Batch 57/782: Batch Loss: 1.0607953071594238\n",
            "Epoch 0/1: Batch 58/782: Batch Loss: 0.84128338098526\n",
            "Epoch 0/1: Batch 59/782: Batch Loss: 0.6311089396476746\n",
            "Epoch 0/1: Batch 60/782: Batch Loss: 0.7484405636787415\n",
            "Epoch 0/1: Batch 61/782: Batch Loss: 0.5448610186576843\n",
            "Epoch 0/1: Batch 62/782: Batch Loss: 0.6110959053039551\n",
            "Epoch 0/1: Batch 63/782: Batch Loss: 0.5493502020835876\n",
            "Epoch 0/1: Batch 64/782: Batch Loss: 0.7474101185798645\n",
            "Epoch 0/1: Batch 65/782: Batch Loss: 0.7935400605201721\n",
            "Epoch 0/1: Batch 66/782: Batch Loss: 0.847930908203125\n",
            "Epoch 0/1: Batch 67/782: Batch Loss: 0.8047139644622803\n",
            "Epoch 0/1: Batch 68/782: Batch Loss: 0.5329864025115967\n",
            "Epoch 0/1: Batch 69/782: Batch Loss: 0.7280381917953491\n",
            "Epoch 0/1: Batch 70/782: Batch Loss: 0.633901834487915\n",
            "Epoch 0/1: Batch 71/782: Batch Loss: 0.6289810538291931\n",
            "Epoch 0/1: Batch 72/782: Batch Loss: 0.48308372497558594\n",
            "Epoch 0/1: Batch 73/782: Batch Loss: 0.6889606714248657\n",
            "Epoch 0/1: Batch 74/782: Batch Loss: 0.6475014686584473\n",
            "Epoch 0/1: Batch 75/782: Batch Loss: 0.7479057312011719\n",
            "Epoch 0/1: Batch 76/782: Batch Loss: 0.6456015706062317\n",
            "Epoch 0/1: Batch 77/782: Batch Loss: 0.7580469250679016\n",
            "Epoch 0/1: Batch 78/782: Batch Loss: 0.8121550679206848\n",
            "Epoch 0/1: Batch 79/782: Batch Loss: 0.7283087372779846\n",
            "Epoch 0/1: Batch 80/782: Batch Loss: 0.7523390650749207\n",
            "Epoch 0/1: Batch 81/782: Batch Loss: 0.6245696544647217\n",
            "Epoch 0/1: Batch 82/782: Batch Loss: 0.6063809990882874\n",
            "Epoch 0/1: Batch 83/782: Batch Loss: 0.8045006990432739\n",
            "Epoch 0/1: Batch 84/782: Batch Loss: 0.5935518741607666\n",
            "Epoch 0/1: Batch 85/782: Batch Loss: 0.554037868976593\n",
            "Epoch 0/1: Batch 86/782: Batch Loss: 0.5913586020469666\n",
            "Epoch 0/1: Batch 87/782: Batch Loss: 0.46726056933403015\n",
            "Epoch 0/1: Batch 88/782: Batch Loss: 0.6472136378288269\n",
            "Epoch 0/1: Batch 89/782: Batch Loss: 0.8104978799819946\n",
            "Epoch 0/1: Batch 90/782: Batch Loss: 0.6358903050422668\n",
            "Epoch 0/1: Batch 91/782: Batch Loss: 0.8003595471382141\n",
            "Epoch 0/1: Batch 92/782: Batch Loss: 0.700208306312561\n",
            "Epoch 0/1: Batch 93/782: Batch Loss: 0.668414294719696\n",
            "Epoch 0/1: Batch 94/782: Batch Loss: 0.7249276041984558\n",
            "Epoch 0/1: Batch 95/782: Batch Loss: 0.655921995639801\n",
            "Epoch 0/1: Batch 96/782: Batch Loss: 0.6443356871604919\n",
            "Epoch 0/1: Batch 97/782: Batch Loss: 0.7152053713798523\n",
            "Epoch 0/1: Batch 98/782: Batch Loss: 0.8426335453987122\n",
            "Epoch 0/1: Batch 99/782: Batch Loss: 0.5321000814437866\n",
            "Epoch 0/1: Batch 100/782: Batch Loss: 0.6085678339004517\n",
            "Epoch 0/1: Batch 101/782: Batch Loss: 0.6277045011520386\n",
            "Epoch 0/1: Batch 102/782: Batch Loss: 0.5871686935424805\n",
            "Epoch 0/1: Batch 103/782: Batch Loss: 0.6420822143554688\n",
            "Epoch 0/1: Batch 104/782: Batch Loss: 0.5438291430473328\n",
            "Epoch 0/1: Batch 105/782: Batch Loss: 0.6757431626319885\n",
            "Epoch 0/1: Batch 106/782: Batch Loss: 0.7350594997406006\n",
            "Epoch 0/1: Batch 107/782: Batch Loss: 0.592641294002533\n",
            "Epoch 0/1: Batch 108/782: Batch Loss: 1.0475893020629883\n",
            "Epoch 0/1: Batch 109/782: Batch Loss: 0.6983315348625183\n",
            "Epoch 0/1: Batch 110/782: Batch Loss: 0.3688206374645233\n",
            "Epoch 0/1: Batch 111/782: Batch Loss: 0.7249452471733093\n",
            "Epoch 0/1: Batch 112/782: Batch Loss: 0.7015726566314697\n",
            "Epoch 0/1: Batch 113/782: Batch Loss: 0.6341192722320557\n",
            "Epoch 0/1: Batch 114/782: Batch Loss: 0.681208074092865\n",
            "Epoch 0/1: Batch 115/782: Batch Loss: 0.41660141944885254\n",
            "Epoch 0/1: Batch 116/782: Batch Loss: 0.7430676221847534\n",
            "Epoch 0/1: Batch 117/782: Batch Loss: 0.7495124340057373\n",
            "Epoch 0/1: Batch 118/782: Batch Loss: 0.5965771675109863\n",
            "Epoch 0/1: Batch 119/782: Batch Loss: 0.4649638533592224\n",
            "Epoch 0/1: Batch 120/782: Batch Loss: 0.5830278396606445\n",
            "Epoch 0/1: Batch 121/782: Batch Loss: 0.86140376329422\n",
            "Epoch 0/1: Batch 122/782: Batch Loss: 0.7885125875473022\n",
            "Epoch 0/1: Batch 123/782: Batch Loss: 0.5235735177993774\n",
            "Epoch 0/1: Batch 124/782: Batch Loss: 0.8527050614356995\n",
            "Epoch 0/1: Batch 125/782: Batch Loss: 0.521105945110321\n",
            "Epoch 0/1: Batch 126/782: Batch Loss: 0.6751613020896912\n",
            "Epoch 0/1: Batch 127/782: Batch Loss: 0.71448814868927\n",
            "Epoch 0/1: Batch 128/782: Batch Loss: 0.7113760709762573\n",
            "Epoch 0/1: Batch 129/782: Batch Loss: 0.6058258414268494\n",
            "Epoch 0/1: Batch 130/782: Batch Loss: 0.8096420764923096\n",
            "Epoch 0/1: Batch 131/782: Batch Loss: 0.847328245639801\n",
            "Epoch 0/1: Batch 132/782: Batch Loss: 0.6534109711647034\n",
            "Epoch 0/1: Batch 133/782: Batch Loss: 0.6494929194450378\n",
            "Epoch 0/1: Batch 134/782: Batch Loss: 0.7793054580688477\n",
            "Epoch 0/1: Batch 135/782: Batch Loss: 0.6907963752746582\n",
            "Epoch 0/1: Batch 136/782: Batch Loss: 0.4878101944923401\n",
            "Epoch 0/1: Batch 137/782: Batch Loss: 0.512296199798584\n",
            "Epoch 0/1: Batch 138/782: Batch Loss: 0.46878954768180847\n",
            "Epoch 0/1: Batch 139/782: Batch Loss: 0.7303935289382935\n",
            "Epoch 0/1: Batch 140/782: Batch Loss: 0.6902782917022705\n",
            "Epoch 0/1: Batch 141/782: Batch Loss: 0.48049452900886536\n",
            "Epoch 0/1: Batch 142/782: Batch Loss: 0.7337874174118042\n",
            "Epoch 0/1: Batch 143/782: Batch Loss: 0.6634610891342163\n",
            "Epoch 0/1: Batch 144/782: Batch Loss: 0.6086499691009521\n",
            "Epoch 0/1: Batch 145/782: Batch Loss: 0.6647897958755493\n",
            "Epoch 0/1: Batch 146/782: Batch Loss: 0.5714741349220276\n",
            "Epoch 0/1: Batch 147/782: Batch Loss: 0.6740732789039612\n",
            "Epoch 0/1: Batch 148/782: Batch Loss: 0.6870490908622742\n",
            "Epoch 0/1: Batch 149/782: Batch Loss: 0.6797956228256226\n",
            "Epoch 0/1: Batch 150/782: Batch Loss: 0.5178223252296448\n",
            "Epoch 0/1: Batch 151/782: Batch Loss: 0.6503851413726807\n",
            "Epoch 0/1: Batch 152/782: Batch Loss: 0.706584095954895\n",
            "Epoch 0/1: Batch 153/782: Batch Loss: 0.5779933929443359\n",
            "Epoch 0/1: Batch 154/782: Batch Loss: 0.5978473424911499\n",
            "Epoch 0/1: Batch 155/782: Batch Loss: 0.7603670358657837\n",
            "Epoch 0/1: Batch 156/782: Batch Loss: 0.6913950443267822\n",
            "Epoch 0/1: Batch 157/782: Batch Loss: 0.5634867548942566\n",
            "Epoch 0/1: Batch 158/782: Batch Loss: 0.6837071180343628\n",
            "Epoch 0/1: Batch 159/782: Batch Loss: 0.9643177390098572\n",
            "Epoch 0/1: Batch 160/782: Batch Loss: 0.7388662099838257\n",
            "Epoch 0/1: Batch 161/782: Batch Loss: 0.8734148740768433\n",
            "Epoch 0/1: Batch 162/782: Batch Loss: 0.46568697690963745\n",
            "Epoch 0/1: Batch 163/782: Batch Loss: 0.714767575263977\n",
            "Epoch 0/1: Batch 164/782: Batch Loss: 0.6715801954269409\n",
            "Epoch 0/1: Batch 165/782: Batch Loss: 0.6148728132247925\n",
            "Epoch 0/1: Batch 166/782: Batch Loss: 0.5541569590568542\n",
            "Epoch 0/1: Batch 167/782: Batch Loss: 0.5477318167686462\n",
            "Epoch 0/1: Batch 168/782: Batch Loss: 0.7949369549751282\n",
            "Epoch 0/1: Batch 169/782: Batch Loss: 0.7240055799484253\n",
            "Epoch 0/1: Batch 170/782: Batch Loss: 0.4912702441215515\n",
            "Epoch 0/1: Batch 171/782: Batch Loss: 0.6915890574455261\n",
            "Epoch 0/1: Batch 172/782: Batch Loss: 0.5483796000480652\n",
            "Epoch 0/1: Batch 173/782: Batch Loss: 0.6076835989952087\n",
            "Epoch 0/1: Batch 174/782: Batch Loss: 0.49725931882858276\n",
            "Epoch 0/1: Batch 175/782: Batch Loss: 1.0558021068572998\n",
            "Epoch 0/1: Batch 176/782: Batch Loss: 0.6563878059387207\n",
            "Epoch 0/1: Batch 177/782: Batch Loss: 0.689224898815155\n",
            "Epoch 0/1: Batch 178/782: Batch Loss: 0.560077428817749\n",
            "Epoch 0/1: Batch 179/782: Batch Loss: 0.43626922369003296\n",
            "Epoch 0/1: Batch 180/782: Batch Loss: 0.6270161867141724\n",
            "Epoch 0/1: Batch 181/782: Batch Loss: 0.6904717683792114\n",
            "Epoch 0/1: Batch 182/782: Batch Loss: 0.7594767212867737\n",
            "Epoch 0/1: Batch 183/782: Batch Loss: 0.8715797662734985\n",
            "Epoch 0/1: Batch 184/782: Batch Loss: 0.8728458285331726\n",
            "Epoch 0/1: Batch 185/782: Batch Loss: 0.6295412182807922\n",
            "Epoch 0/1: Batch 186/782: Batch Loss: 0.525244951248169\n",
            "Epoch 0/1: Batch 187/782: Batch Loss: 0.7818798422813416\n",
            "Epoch 0/1: Batch 188/782: Batch Loss: 0.44155266880989075\n",
            "Epoch 0/1: Batch 189/782: Batch Loss: 0.46797508001327515\n",
            "Epoch 0/1: Batch 190/782: Batch Loss: 0.5660869479179382\n",
            "Epoch 0/1: Batch 191/782: Batch Loss: 0.6980181336402893\n",
            "Epoch 0/1: Batch 192/782: Batch Loss: 0.5852996706962585\n",
            "Epoch 0/1: Batch 193/782: Batch Loss: 0.5768201947212219\n",
            "Epoch 0/1: Batch 194/782: Batch Loss: 0.5079218745231628\n",
            "Epoch 0/1: Batch 195/782: Batch Loss: 0.8701309561729431\n",
            "Epoch 0/1: Batch 196/782: Batch Loss: 0.6136859059333801\n",
            "Epoch 0/1: Batch 197/782: Batch Loss: 0.6375975012779236\n",
            "Epoch 0/1: Batch 198/782: Batch Loss: 0.5093220472335815\n",
            "Epoch 0/1: Batch 199/782: Batch Loss: 0.6219637989997864\n",
            "Epoch 0/1: Batch 200/782: Batch Loss: 0.5496411323547363\n",
            "Epoch 0/1: Batch 201/782: Batch Loss: 0.4504687488079071\n",
            "Epoch 0/1: Batch 202/782: Batch Loss: 0.5892671942710876\n",
            "Epoch 0/1: Batch 203/782: Batch Loss: 0.4803578853607178\n",
            "Epoch 0/1: Batch 204/782: Batch Loss: 0.6556379795074463\n",
            "Epoch 0/1: Batch 205/782: Batch Loss: 0.6579563617706299\n",
            "Epoch 0/1: Batch 206/782: Batch Loss: 0.5383187532424927\n",
            "Epoch 0/1: Batch 207/782: Batch Loss: 0.7278748750686646\n",
            "Epoch 0/1: Batch 208/782: Batch Loss: 0.7381634712219238\n",
            "Epoch 0/1: Batch 209/782: Batch Loss: 0.47631001472473145\n",
            "Epoch 0/1: Batch 210/782: Batch Loss: 0.7393016219139099\n",
            "Epoch 0/1: Batch 211/782: Batch Loss: 0.8463202118873596\n",
            "Epoch 0/1: Batch 212/782: Batch Loss: 0.7511499524116516\n",
            "Epoch 0/1: Batch 213/782: Batch Loss: 0.5542720556259155\n",
            "Epoch 0/1: Batch 214/782: Batch Loss: 0.590593695640564\n",
            "Epoch 0/1: Batch 215/782: Batch Loss: 0.6236810684204102\n",
            "Epoch 0/1: Batch 216/782: Batch Loss: 0.5767285823822021\n",
            "Epoch 0/1: Batch 217/782: Batch Loss: 0.47866329550743103\n",
            "Epoch 0/1: Batch 218/782: Batch Loss: 0.5152900218963623\n",
            "Epoch 0/1: Batch 219/782: Batch Loss: 0.6628606915473938\n",
            "Epoch 0/1: Batch 220/782: Batch Loss: 0.586205005645752\n",
            "Epoch 0/1: Batch 221/782: Batch Loss: 0.7937235236167908\n",
            "Epoch 0/1: Batch 222/782: Batch Loss: 0.5672897696495056\n",
            "Epoch 0/1: Batch 223/782: Batch Loss: 0.755874752998352\n",
            "Epoch 0/1: Batch 224/782: Batch Loss: 0.727904736995697\n",
            "Epoch 0/1: Batch 225/782: Batch Loss: 0.3744942843914032\n",
            "Epoch 0/1: Batch 226/782: Batch Loss: 0.5361173152923584\n",
            "Epoch 0/1: Batch 227/782: Batch Loss: 0.6969409584999084\n",
            "Epoch 0/1: Batch 228/782: Batch Loss: 0.3958495259284973\n",
            "Epoch 0/1: Batch 229/782: Batch Loss: 0.5701284408569336\n",
            "Epoch 0/1: Batch 230/782: Batch Loss: 0.6387989521026611\n",
            "Epoch 0/1: Batch 231/782: Batch Loss: 0.5618626475334167\n",
            "Epoch 0/1: Batch 232/782: Batch Loss: 0.5219088792800903\n",
            "Epoch 0/1: Batch 233/782: Batch Loss: 0.4370574951171875\n",
            "Epoch 0/1: Batch 234/782: Batch Loss: 0.6987119317054749\n",
            "Epoch 0/1: Batch 235/782: Batch Loss: 0.6295186877250671\n",
            "Epoch 0/1: Batch 236/782: Batch Loss: 0.6736668944358826\n",
            "Epoch 0/1: Batch 237/782: Batch Loss: 0.7859653830528259\n",
            "Epoch 0/1: Batch 238/782: Batch Loss: 0.7673367261886597\n",
            "Epoch 0/1: Batch 239/782: Batch Loss: 0.6462990641593933\n",
            "Epoch 0/1: Batch 240/782: Batch Loss: 0.6106460094451904\n",
            "Epoch 0/1: Batch 241/782: Batch Loss: 0.6633453369140625\n",
            "Epoch 0/1: Batch 242/782: Batch Loss: 0.6577481031417847\n",
            "Epoch 0/1: Batch 243/782: Batch Loss: 0.5197626352310181\n",
            "Epoch 0/1: Batch 244/782: Batch Loss: 0.3027404844760895\n",
            "Epoch 0/1: Batch 245/782: Batch Loss: 0.5610619187355042\n",
            "Epoch 0/1: Batch 246/782: Batch Loss: 0.8298524618148804\n",
            "Epoch 0/1: Batch 247/782: Batch Loss: 0.7118895053863525\n",
            "Epoch 0/1: Batch 248/782: Batch Loss: 0.5774663686752319\n",
            "Epoch 0/1: Batch 249/782: Batch Loss: 0.6132590770721436\n",
            "Epoch 0/1: Batch 250/782: Batch Loss: 0.7618468999862671\n",
            "Epoch 0/1: Batch 251/782: Batch Loss: 0.5201215744018555\n",
            "Epoch 0/1: Batch 252/782: Batch Loss: 0.4164368212223053\n",
            "Epoch 0/1: Batch 253/782: Batch Loss: 0.8030179738998413\n",
            "Epoch 0/1: Batch 254/782: Batch Loss: 0.6989163160324097\n",
            "Epoch 0/1: Batch 255/782: Batch Loss: 0.542708694934845\n",
            "Epoch 0/1: Batch 256/782: Batch Loss: 0.6625467538833618\n",
            "Epoch 0/1: Batch 257/782: Batch Loss: 0.7036983966827393\n",
            "Epoch 0/1: Batch 258/782: Batch Loss: 0.5535182952880859\n",
            "Epoch 0/1: Batch 259/782: Batch Loss: 0.42509058117866516\n",
            "Epoch 0/1: Batch 260/782: Batch Loss: 0.622319221496582\n",
            "Epoch 0/1: Batch 261/782: Batch Loss: 0.527138888835907\n",
            "Epoch 0/1: Batch 262/782: Batch Loss: 0.7568226456642151\n",
            "Epoch 0/1: Batch 263/782: Batch Loss: 0.6762053966522217\n",
            "Epoch 0/1: Batch 264/782: Batch Loss: 0.6200548410415649\n",
            "Epoch 0/1: Batch 265/782: Batch Loss: 0.5485930442810059\n",
            "Epoch 0/1: Batch 266/782: Batch Loss: 0.6781135201454163\n",
            "Epoch 0/1: Batch 267/782: Batch Loss: 0.6053810119628906\n",
            "Epoch 0/1: Batch 268/782: Batch Loss: 0.6193013191223145\n",
            "Epoch 0/1: Batch 269/782: Batch Loss: 0.5825363397598267\n",
            "Epoch 0/1: Batch 270/782: Batch Loss: 0.4328702688217163\n",
            "Epoch 0/1: Batch 271/782: Batch Loss: 0.6450180411338806\n",
            "Epoch 0/1: Batch 272/782: Batch Loss: 0.5986272096633911\n",
            "Epoch 0/1: Batch 273/782: Batch Loss: 0.5909214019775391\n",
            "Epoch 0/1: Batch 274/782: Batch Loss: 0.5079995393753052\n",
            "Epoch 0/1: Batch 275/782: Batch Loss: 0.7097892165184021\n",
            "Epoch 0/1: Batch 276/782: Batch Loss: 0.5616196990013123\n",
            "Epoch 0/1: Batch 277/782: Batch Loss: 0.6532736420631409\n",
            "Epoch 0/1: Batch 278/782: Batch Loss: 0.6936265826225281\n",
            "Epoch 0/1: Batch 279/782: Batch Loss: 0.494174063205719\n",
            "Epoch 0/1: Batch 280/782: Batch Loss: 0.5628436803817749\n",
            "Epoch 0/1: Batch 281/782: Batch Loss: 0.5568992495536804\n",
            "Epoch 0/1: Batch 282/782: Batch Loss: 0.5604044795036316\n",
            "Epoch 0/1: Batch 283/782: Batch Loss: 0.6044106483459473\n",
            "Epoch 0/1: Batch 284/782: Batch Loss: 0.4501192271709442\n",
            "Epoch 0/1: Batch 285/782: Batch Loss: 0.7207653522491455\n",
            "Epoch 0/1: Batch 286/782: Batch Loss: 0.5171870589256287\n",
            "Epoch 0/1: Batch 287/782: Batch Loss: 0.5293540954589844\n",
            "Epoch 0/1: Batch 288/782: Batch Loss: 0.5608772039413452\n",
            "Epoch 0/1: Batch 289/782: Batch Loss: 0.5333653688430786\n",
            "Epoch 0/1: Batch 290/782: Batch Loss: 1.0496655702590942\n",
            "Epoch 0/1: Batch 291/782: Batch Loss: 0.5920068025588989\n",
            "Epoch 0/1: Batch 292/782: Batch Loss: 0.7554882764816284\n",
            "Epoch 0/1: Batch 293/782: Batch Loss: 0.5857799053192139\n",
            "Epoch 0/1: Batch 294/782: Batch Loss: 0.47691696882247925\n",
            "Epoch 0/1: Batch 295/782: Batch Loss: 0.83790522813797\n",
            "Epoch 0/1: Batch 296/782: Batch Loss: 0.6443537473678589\n",
            "Epoch 0/1: Batch 297/782: Batch Loss: 0.5683433413505554\n",
            "Epoch 0/1: Batch 298/782: Batch Loss: 0.891152560710907\n",
            "Epoch 0/1: Batch 299/782: Batch Loss: 0.6582525372505188\n",
            "Epoch 0/1: Batch 300/782: Batch Loss: 0.5282438397407532\n",
            "Epoch 0/1: Batch 301/782: Batch Loss: 0.7102638483047485\n",
            "Epoch 0/1: Batch 302/782: Batch Loss: 0.5876578688621521\n",
            "Epoch 0/1: Batch 303/782: Batch Loss: 0.4872097671031952\n",
            "Epoch 0/1: Batch 304/782: Batch Loss: 0.45130303502082825\n",
            "Epoch 0/1: Batch 305/782: Batch Loss: 0.49215757846832275\n",
            "Epoch 0/1: Batch 306/782: Batch Loss: 0.9619463682174683\n",
            "Epoch 0/1: Batch 307/782: Batch Loss: 0.5066670775413513\n",
            "Epoch 0/1: Batch 308/782: Batch Loss: 0.5236901640892029\n",
            "Epoch 0/1: Batch 309/782: Batch Loss: 0.6561936140060425\n",
            "Epoch 0/1: Batch 310/782: Batch Loss: 0.6179929971694946\n",
            "Epoch 0/1: Batch 311/782: Batch Loss: 0.747292697429657\n",
            "Epoch 0/1: Batch 312/782: Batch Loss: 0.7363735437393188\n",
            "Epoch 0/1: Batch 313/782: Batch Loss: 0.6551880836486816\n",
            "Epoch 0/1: Batch 314/782: Batch Loss: 0.4669438302516937\n",
            "Epoch 0/1: Batch 315/782: Batch Loss: 0.609978437423706\n",
            "Epoch 0/1: Batch 316/782: Batch Loss: 0.3513243794441223\n",
            "Epoch 0/1: Batch 317/782: Batch Loss: 0.6058422923088074\n",
            "Epoch 0/1: Batch 318/782: Batch Loss: 0.5194668769836426\n",
            "Epoch 0/1: Batch 319/782: Batch Loss: 0.5921967625617981\n",
            "Epoch 0/1: Batch 320/782: Batch Loss: 0.4115520119667053\n",
            "Epoch 0/1: Batch 321/782: Batch Loss: 0.5059996843338013\n",
            "Epoch 0/1: Batch 322/782: Batch Loss: 0.611929714679718\n",
            "Epoch 0/1: Batch 323/782: Batch Loss: 0.4280776381492615\n",
            "Epoch 0/1: Batch 324/782: Batch Loss: 0.709926426410675\n",
            "Epoch 0/1: Batch 325/782: Batch Loss: 0.629395067691803\n",
            "Epoch 0/1: Batch 326/782: Batch Loss: 0.5680193305015564\n",
            "Epoch 0/1: Batch 327/782: Batch Loss: 0.5743126273155212\n",
            "Epoch 0/1: Batch 328/782: Batch Loss: 0.5477762818336487\n",
            "Epoch 0/1: Batch 329/782: Batch Loss: 0.6365289688110352\n",
            "Epoch 0/1: Batch 330/782: Batch Loss: 0.5983803868293762\n",
            "Epoch 0/1: Batch 331/782: Batch Loss: 0.7843969464302063\n",
            "Epoch 0/1: Batch 332/782: Batch Loss: 0.6690825819969177\n",
            "Epoch 0/1: Batch 333/782: Batch Loss: 0.5971150994300842\n",
            "Epoch 0/1: Batch 334/782: Batch Loss: 0.8957529067993164\n",
            "Epoch 0/1: Batch 335/782: Batch Loss: 0.4145796298980713\n",
            "Epoch 0/1: Batch 336/782: Batch Loss: 0.8783875703811646\n",
            "Epoch 0/1: Batch 337/782: Batch Loss: 0.6286396980285645\n",
            "Epoch 0/1: Batch 338/782: Batch Loss: 0.5336388945579529\n",
            "Epoch 0/1: Batch 339/782: Batch Loss: 0.8281591534614563\n",
            "Epoch 0/1: Batch 340/782: Batch Loss: 0.5445376038551331\n",
            "Epoch 0/1: Batch 341/782: Batch Loss: 0.8564003705978394\n",
            "Epoch 0/1: Batch 342/782: Batch Loss: 0.7434643507003784\n",
            "Epoch 0/1: Batch 343/782: Batch Loss: 0.6082093119621277\n",
            "Epoch 0/1: Batch 344/782: Batch Loss: 0.45820122957229614\n",
            "Epoch 0/1: Batch 345/782: Batch Loss: 0.6612536311149597\n",
            "Epoch 0/1: Batch 346/782: Batch Loss: 0.6309077739715576\n",
            "Epoch 0/1: Batch 347/782: Batch Loss: 0.581470787525177\n",
            "Epoch 0/1: Batch 348/782: Batch Loss: 0.7075406312942505\n",
            "Epoch 0/1: Batch 349/782: Batch Loss: 0.46360987424850464\n",
            "Epoch 0/1: Batch 350/782: Batch Loss: 0.7110126614570618\n",
            "Epoch 0/1: Batch 351/782: Batch Loss: 0.8432621359825134\n",
            "Epoch 0/1: Batch 352/782: Batch Loss: 0.7358798384666443\n",
            "Epoch 0/1: Batch 353/782: Batch Loss: 0.7083345055580139\n",
            "Epoch 0/1: Batch 354/782: Batch Loss: 0.5307267308235168\n",
            "Epoch 0/1: Batch 355/782: Batch Loss: 1.0549869537353516\n",
            "Epoch 0/1: Batch 356/782: Batch Loss: 0.5668190121650696\n",
            "Epoch 0/1: Batch 357/782: Batch Loss: 0.5282192826271057\n",
            "Epoch 0/1: Batch 358/782: Batch Loss: 0.5781494975090027\n",
            "Epoch 0/1: Batch 359/782: Batch Loss: 0.6288179755210876\n",
            "Epoch 0/1: Batch 360/782: Batch Loss: 0.9376932978630066\n",
            "Epoch 0/1: Batch 361/782: Batch Loss: 0.5950471758842468\n",
            "Epoch 0/1: Batch 362/782: Batch Loss: 0.7014894485473633\n",
            "Epoch 0/1: Batch 363/782: Batch Loss: 0.584480345249176\n",
            "Epoch 0/1: Batch 364/782: Batch Loss: 0.6034996509552002\n",
            "Epoch 0/1: Batch 365/782: Batch Loss: 0.7213921546936035\n",
            "Epoch 0/1: Batch 366/782: Batch Loss: 0.41486144065856934\n",
            "Epoch 0/1: Batch 367/782: Batch Loss: 0.6705241203308105\n",
            "Epoch 0/1: Batch 368/782: Batch Loss: 0.5151358842849731\n",
            "Epoch 0/1: Batch 369/782: Batch Loss: 0.70361328125\n",
            "Epoch 0/1: Batch 370/782: Batch Loss: 0.7778645157814026\n",
            "Epoch 0/1: Batch 371/782: Batch Loss: 0.851632297039032\n",
            "Epoch 0/1: Batch 372/782: Batch Loss: 0.6756618022918701\n",
            "Epoch 0/1: Batch 373/782: Batch Loss: 0.5735334157943726\n",
            "Epoch 0/1: Batch 374/782: Batch Loss: 0.6181299686431885\n",
            "Epoch 0/1: Batch 375/782: Batch Loss: 0.6503056883811951\n",
            "Epoch 0/1: Batch 376/782: Batch Loss: 0.5540741086006165\n",
            "Epoch 0/1: Batch 377/782: Batch Loss: 0.8569185137748718\n",
            "Epoch 0/1: Batch 378/782: Batch Loss: 0.6344971656799316\n",
            "Epoch 0/1: Batch 379/782: Batch Loss: 0.6893588304519653\n",
            "Epoch 0/1: Batch 380/782: Batch Loss: 0.2902298867702484\n",
            "Epoch 0/1: Batch 381/782: Batch Loss: 0.4234408140182495\n",
            "Epoch 0/1: Batch 382/782: Batch Loss: 0.6378979086875916\n",
            "Epoch 0/1: Batch 383/782: Batch Loss: 0.4131898880004883\n",
            "Epoch 0/1: Batch 384/782: Batch Loss: 0.6166259050369263\n",
            "Epoch 0/1: Batch 385/782: Batch Loss: 0.703569769859314\n",
            "Epoch 0/1: Batch 386/782: Batch Loss: 0.6241305470466614\n",
            "Epoch 0/1: Batch 387/782: Batch Loss: 0.5657740235328674\n",
            "Epoch 0/1: Batch 388/782: Batch Loss: 0.6131911873817444\n",
            "Epoch 0/1: Batch 389/782: Batch Loss: 0.6902677416801453\n",
            "Epoch 0/1: Batch 390/782: Batch Loss: 0.5510708689689636\n",
            "Epoch 0/1: Batch 391/782: Batch Loss: 0.6976948976516724\n",
            "Epoch 0/1: Batch 392/782: Batch Loss: 0.6748389005661011\n",
            "Epoch 0/1: Batch 393/782: Batch Loss: 0.6524881720542908\n",
            "Epoch 0/1: Batch 394/782: Batch Loss: 0.519023597240448\n",
            "Epoch 0/1: Batch 395/782: Batch Loss: 0.4715980887413025\n",
            "Epoch 0/1: Batch 396/782: Batch Loss: 0.840785562992096\n",
            "Epoch 0/1: Batch 397/782: Batch Loss: 0.5126845240592957\n",
            "Epoch 0/1: Batch 398/782: Batch Loss: 0.7290343046188354\n",
            "Epoch 0/1: Batch 399/782: Batch Loss: 0.7372103333473206\n",
            "Epoch 0/1: Batch 400/782: Batch Loss: 0.47556978464126587\n",
            "Epoch 0/1: Batch 401/782: Batch Loss: 0.5440537929534912\n",
            "Epoch 0/1: Batch 402/782: Batch Loss: 0.47941839694976807\n",
            "Epoch 0/1: Batch 403/782: Batch Loss: 0.7903315424919128\n",
            "Epoch 0/1: Batch 404/782: Batch Loss: 0.465257853269577\n",
            "Epoch 0/1: Batch 405/782: Batch Loss: 0.5800447463989258\n",
            "Epoch 0/1: Batch 406/782: Batch Loss: 0.6300452351570129\n",
            "Epoch 0/1: Batch 407/782: Batch Loss: 0.7847955226898193\n",
            "Epoch 0/1: Batch 408/782: Batch Loss: 0.6049520969390869\n",
            "Epoch 0/1: Batch 409/782: Batch Loss: 0.7009121179580688\n",
            "Epoch 0/1: Batch 410/782: Batch Loss: 0.5964710712432861\n",
            "Epoch 0/1: Batch 411/782: Batch Loss: 0.7254770398139954\n",
            "Epoch 0/1: Batch 412/782: Batch Loss: 0.5536445379257202\n",
            "Epoch 0/1: Batch 413/782: Batch Loss: 0.5206021070480347\n",
            "Epoch 0/1: Batch 414/782: Batch Loss: 0.6859464049339294\n",
            "Epoch 0/1: Batch 415/782: Batch Loss: 0.557502269744873\n",
            "Epoch 0/1: Batch 416/782: Batch Loss: 0.5282861590385437\n",
            "Epoch 0/1: Batch 417/782: Batch Loss: 0.3623471260070801\n",
            "Epoch 0/1: Batch 418/782: Batch Loss: 0.5744500160217285\n",
            "Epoch 0/1: Batch 419/782: Batch Loss: 0.8292933106422424\n",
            "Epoch 0/1: Batch 420/782: Batch Loss: 0.5744554400444031\n",
            "Epoch 0/1: Batch 421/782: Batch Loss: 0.9491565823554993\n",
            "Epoch 0/1: Batch 422/782: Batch Loss: 0.5282390117645264\n",
            "Epoch 0/1: Batch 423/782: Batch Loss: 0.6681851744651794\n",
            "Epoch 0/1: Batch 424/782: Batch Loss: 0.6649190783500671\n",
            "Epoch 0/1: Batch 425/782: Batch Loss: 0.5721145868301392\n",
            "Epoch 0/1: Batch 426/782: Batch Loss: 0.4279339909553528\n",
            "Epoch 0/1: Batch 427/782: Batch Loss: 0.630125105381012\n",
            "Epoch 0/1: Batch 428/782: Batch Loss: 0.516308605670929\n",
            "Epoch 0/1: Batch 429/782: Batch Loss: 0.5194476246833801\n",
            "Epoch 0/1: Batch 430/782: Batch Loss: 0.6036220788955688\n",
            "Epoch 0/1: Batch 431/782: Batch Loss: 0.7079872488975525\n",
            "Epoch 0/1: Batch 432/782: Batch Loss: 0.4235517978668213\n",
            "Epoch 0/1: Batch 433/782: Batch Loss: 0.5611369609832764\n",
            "Epoch 0/1: Batch 434/782: Batch Loss: 0.7501947283744812\n",
            "Epoch 0/1: Batch 435/782: Batch Loss: 0.6441206932067871\n",
            "Epoch 0/1: Batch 436/782: Batch Loss: 0.7187238931655884\n",
            "Epoch 0/1: Batch 437/782: Batch Loss: 0.5759208798408508\n",
            "Epoch 0/1: Batch 438/782: Batch Loss: 0.6318494081497192\n",
            "Epoch 0/1: Batch 439/782: Batch Loss: 0.5471671223640442\n",
            "Epoch 0/1: Batch 440/782: Batch Loss: 0.3751155436038971\n",
            "Epoch 0/1: Batch 441/782: Batch Loss: 0.545927107334137\n",
            "Epoch 0/1: Batch 442/782: Batch Loss: 0.5799627900123596\n",
            "Epoch 0/1: Batch 443/782: Batch Loss: 0.5765315294265747\n",
            "Epoch 0/1: Batch 444/782: Batch Loss: 0.7557498216629028\n",
            "Epoch 0/1: Batch 445/782: Batch Loss: 0.6289151310920715\n",
            "Epoch 0/1: Batch 446/782: Batch Loss: 0.6916465163230896\n",
            "Epoch 0/1: Batch 447/782: Batch Loss: 0.7266767621040344\n",
            "Epoch 0/1: Batch 448/782: Batch Loss: 0.294106125831604\n",
            "Epoch 0/1: Batch 449/782: Batch Loss: 0.520046055316925\n",
            "Epoch 0/1: Batch 450/782: Batch Loss: 0.5954543352127075\n",
            "Epoch 0/1: Batch 451/782: Batch Loss: 0.9300867915153503\n",
            "Epoch 0/1: Batch 452/782: Batch Loss: 0.46721139550209045\n",
            "Epoch 0/1: Batch 453/782: Batch Loss: 0.6788136959075928\n",
            "Epoch 0/1: Batch 454/782: Batch Loss: 0.5633582472801208\n",
            "Epoch 0/1: Batch 455/782: Batch Loss: 0.5028113722801208\n",
            "Epoch 0/1: Batch 456/782: Batch Loss: 0.5058373212814331\n",
            "Epoch 0/1: Batch 457/782: Batch Loss: 0.8381811380386353\n",
            "Epoch 0/1: Batch 458/782: Batch Loss: 0.4977273941040039\n",
            "Epoch 0/1: Batch 459/782: Batch Loss: 0.6319926977157593\n",
            "Epoch 0/1: Batch 460/782: Batch Loss: 0.40819188952445984\n",
            "Epoch 0/1: Batch 461/782: Batch Loss: 0.6386319398880005\n",
            "Epoch 0/1: Batch 462/782: Batch Loss: 0.6352637410163879\n",
            "Epoch 0/1: Batch 463/782: Batch Loss: 0.733016848564148\n",
            "Epoch 0/1: Batch 464/782: Batch Loss: 0.5514568090438843\n",
            "Epoch 0/1: Batch 465/782: Batch Loss: 1.036602258682251\n",
            "Epoch 0/1: Batch 466/782: Batch Loss: 0.5257712602615356\n",
            "Epoch 0/1: Batch 467/782: Batch Loss: 0.44425472617149353\n",
            "Epoch 0/1: Batch 468/782: Batch Loss: 0.8476026654243469\n",
            "Epoch 0/1: Batch 469/782: Batch Loss: 0.6196902394294739\n",
            "Epoch 0/1: Batch 470/782: Batch Loss: 0.37368300557136536\n",
            "Epoch 0/1: Batch 471/782: Batch Loss: 0.5396822690963745\n",
            "Epoch 0/1: Batch 472/782: Batch Loss: 0.5975633263587952\n",
            "Epoch 0/1: Batch 473/782: Batch Loss: 0.6565941572189331\n",
            "Epoch 0/1: Batch 474/782: Batch Loss: 0.452102929353714\n",
            "Epoch 0/1: Batch 475/782: Batch Loss: 0.6577932834625244\n",
            "Epoch 0/1: Batch 476/782: Batch Loss: 0.500575602054596\n",
            "Epoch 0/1: Batch 477/782: Batch Loss: 0.6056408286094666\n",
            "Epoch 0/1: Batch 478/782: Batch Loss: 0.3695107102394104\n",
            "Epoch 0/1: Batch 479/782: Batch Loss: 0.7947803139686584\n",
            "Epoch 0/1: Batch 480/782: Batch Loss: 0.8368433713912964\n",
            "Epoch 0/1: Batch 481/782: Batch Loss: 0.638826847076416\n",
            "Epoch 0/1: Batch 482/782: Batch Loss: 0.8492690324783325\n",
            "Epoch 0/1: Batch 483/782: Batch Loss: 0.6995678544044495\n",
            "Epoch 0/1: Batch 484/782: Batch Loss: 0.6889743208885193\n",
            "Epoch 0/1: Batch 485/782: Batch Loss: 0.8308919072151184\n",
            "Epoch 0/1: Batch 486/782: Batch Loss: 0.4569506347179413\n",
            "Epoch 0/1: Batch 487/782: Batch Loss: 0.553885281085968\n",
            "Epoch 0/1: Batch 488/782: Batch Loss: 0.697221040725708\n",
            "Epoch 0/1: Batch 489/782: Batch Loss: 0.6467213034629822\n",
            "Epoch 0/1: Batch 490/782: Batch Loss: 0.5089762210845947\n",
            "Epoch 0/1: Batch 491/782: Batch Loss: 0.5816761255264282\n",
            "Epoch 0/1: Batch 492/782: Batch Loss: 0.7606574892997742\n",
            "Epoch 0/1: Batch 493/782: Batch Loss: 0.4086035192012787\n",
            "Epoch 0/1: Batch 494/782: Batch Loss: 0.7039821147918701\n",
            "Epoch 0/1: Batch 495/782: Batch Loss: 0.6550927758216858\n",
            "Epoch 0/1: Batch 496/782: Batch Loss: 0.5543043613433838\n",
            "Epoch 0/1: Batch 497/782: Batch Loss: 0.6462811827659607\n",
            "Epoch 0/1: Batch 498/782: Batch Loss: 0.4353657066822052\n",
            "Epoch 0/1: Batch 499/782: Batch Loss: 0.4905020296573639\n",
            "Epoch 0/1: Batch 500/782: Batch Loss: 0.5682644844055176\n",
            "Epoch 0/1: Batch 501/782: Batch Loss: 0.5579709410667419\n",
            "Epoch 0/1: Batch 502/782: Batch Loss: 0.5434250235557556\n",
            "Epoch 0/1: Batch 503/782: Batch Loss: 0.5332813858985901\n",
            "Epoch 0/1: Batch 504/782: Batch Loss: 0.49069923162460327\n",
            "Epoch 0/1: Batch 505/782: Batch Loss: 0.7924575805664062\n",
            "Epoch 0/1: Batch 506/782: Batch Loss: 0.46252840757369995\n",
            "Epoch 0/1: Batch 507/782: Batch Loss: 0.7016459107398987\n",
            "Epoch 0/1: Batch 508/782: Batch Loss: 0.8105838894844055\n",
            "Epoch 0/1: Batch 509/782: Batch Loss: 0.5860766172409058\n",
            "Epoch 0/1: Batch 510/782: Batch Loss: 0.8326178193092346\n",
            "Epoch 0/1: Batch 511/782: Batch Loss: 0.8330724239349365\n",
            "Epoch 0/1: Batch 512/782: Batch Loss: 0.41826319694519043\n",
            "Epoch 0/1: Batch 513/782: Batch Loss: 0.6545571088790894\n",
            "Epoch 0/1: Batch 514/782: Batch Loss: 0.5526033043861389\n",
            "Epoch 0/1: Batch 515/782: Batch Loss: 0.45366576313972473\n",
            "Epoch 0/1: Batch 516/782: Batch Loss: 0.7587651014328003\n",
            "Epoch 0/1: Batch 517/782: Batch Loss: 0.8849576711654663\n",
            "Epoch 0/1: Batch 518/782: Batch Loss: 0.6578466296195984\n",
            "Epoch 0/1: Batch 519/782: Batch Loss: 0.4430573582649231\n",
            "Epoch 0/1: Batch 520/782: Batch Loss: 0.5972025394439697\n",
            "Epoch 0/1: Batch 521/782: Batch Loss: 0.8264350295066833\n",
            "Epoch 0/1: Batch 522/782: Batch Loss: 0.6436426639556885\n",
            "Epoch 0/1: Batch 523/782: Batch Loss: 0.5842911601066589\n",
            "Epoch 0/1: Batch 524/782: Batch Loss: 0.5202897787094116\n",
            "Epoch 0/1: Batch 525/782: Batch Loss: 0.7575216293334961\n",
            "Epoch 0/1: Batch 526/782: Batch Loss: 0.7215463519096375\n",
            "Epoch 0/1: Batch 527/782: Batch Loss: 0.5380431413650513\n",
            "Epoch 0/1: Batch 528/782: Batch Loss: 0.6992251873016357\n",
            "Epoch 0/1: Batch 529/782: Batch Loss: 0.7930238842964172\n",
            "Epoch 0/1: Batch 530/782: Batch Loss: 0.6019229292869568\n",
            "Epoch 0/1: Batch 531/782: Batch Loss: 0.6685314178466797\n",
            "Epoch 0/1: Batch 532/782: Batch Loss: 0.6075499653816223\n",
            "Epoch 0/1: Batch 533/782: Batch Loss: 0.6300913691520691\n",
            "Epoch 0/1: Batch 534/782: Batch Loss: 0.6259078979492188\n",
            "Epoch 0/1: Batch 535/782: Batch Loss: 0.6653276085853577\n",
            "Epoch 0/1: Batch 536/782: Batch Loss: 0.7235151529312134\n",
            "Epoch 0/1: Batch 537/782: Batch Loss: 0.6899393200874329\n",
            "Epoch 0/1: Batch 538/782: Batch Loss: 0.4759865403175354\n",
            "Epoch 0/1: Batch 539/782: Batch Loss: 0.5852231979370117\n",
            "Epoch 0/1: Batch 540/782: Batch Loss: 0.5967934131622314\n",
            "Epoch 0/1: Batch 541/782: Batch Loss: 0.4734669327735901\n",
            "Epoch 0/1: Batch 542/782: Batch Loss: 0.658930778503418\n",
            "Epoch 0/1: Batch 543/782: Batch Loss: 0.5584456920623779\n",
            "Epoch 0/1: Batch 544/782: Batch Loss: 0.6245577335357666\n",
            "Epoch 0/1: Batch 545/782: Batch Loss: 0.4548693597316742\n",
            "Epoch 0/1: Batch 546/782: Batch Loss: 0.8054590225219727\n",
            "Epoch 0/1: Batch 547/782: Batch Loss: 0.39785727858543396\n",
            "Epoch 0/1: Batch 548/782: Batch Loss: 0.5812751054763794\n",
            "Epoch 0/1: Batch 549/782: Batch Loss: 0.5475478172302246\n",
            "Epoch 0/1: Batch 550/782: Batch Loss: 0.49384617805480957\n",
            "Epoch 0/1: Batch 551/782: Batch Loss: 0.7165989875793457\n",
            "Epoch 0/1: Batch 552/782: Batch Loss: 0.5798376202583313\n",
            "Epoch 0/1: Batch 553/782: Batch Loss: 0.5776867866516113\n",
            "Epoch 0/1: Batch 554/782: Batch Loss: 0.6635642051696777\n",
            "Epoch 0/1: Batch 555/782: Batch Loss: 0.7655209898948669\n",
            "Epoch 0/1: Batch 556/782: Batch Loss: 0.6904398202896118\n",
            "Epoch 0/1: Batch 557/782: Batch Loss: 0.7153019309043884\n",
            "Epoch 0/1: Batch 558/782: Batch Loss: 0.4676695466041565\n",
            "Epoch 0/1: Batch 559/782: Batch Loss: 0.4742456078529358\n",
            "Epoch 0/1: Batch 560/782: Batch Loss: 0.649071455001831\n",
            "Epoch 0/1: Batch 561/782: Batch Loss: 1.0203808546066284\n",
            "Epoch 0/1: Batch 562/782: Batch Loss: 0.6451597809791565\n",
            "Epoch 0/1: Batch 563/782: Batch Loss: 0.5031011700630188\n",
            "Epoch 0/1: Batch 564/782: Batch Loss: 0.7582593560218811\n",
            "Epoch 0/1: Batch 565/782: Batch Loss: 0.5330315232276917\n",
            "Epoch 0/1: Batch 566/782: Batch Loss: 0.8783281445503235\n",
            "Epoch 0/1: Batch 567/782: Batch Loss: 0.71004718542099\n",
            "Epoch 0/1: Batch 568/782: Batch Loss: 0.6569657921791077\n",
            "Epoch 0/1: Batch 569/782: Batch Loss: 0.4531024694442749\n",
            "Epoch 0/1: Batch 570/782: Batch Loss: 0.5707844495773315\n",
            "Epoch 0/1: Batch 571/782: Batch Loss: 1.0371906757354736\n",
            "Epoch 0/1: Batch 572/782: Batch Loss: 0.8226898312568665\n",
            "Epoch 0/1: Batch 573/782: Batch Loss: 0.6454225778579712\n",
            "Epoch 0/1: Batch 574/782: Batch Loss: 0.4835897386074066\n",
            "Epoch 0/1: Batch 575/782: Batch Loss: 0.6745181679725647\n",
            "Epoch 0/1: Batch 576/782: Batch Loss: 0.5433162450790405\n",
            "Epoch 0/1: Batch 577/782: Batch Loss: 0.7973896265029907\n",
            "Epoch 0/1: Batch 578/782: Batch Loss: 0.7109565138816833\n",
            "Epoch 0/1: Batch 579/782: Batch Loss: 0.6825994253158569\n",
            "Epoch 0/1: Batch 580/782: Batch Loss: 0.6516028642654419\n",
            "Epoch 0/1: Batch 581/782: Batch Loss: 0.6985518932342529\n",
            "Epoch 0/1: Batch 582/782: Batch Loss: 0.8456977605819702\n",
            "Epoch 0/1: Batch 583/782: Batch Loss: 0.6967958211898804\n",
            "Epoch 0/1: Batch 584/782: Batch Loss: 0.3122069835662842\n",
            "Epoch 0/1: Batch 585/782: Batch Loss: 0.6208009719848633\n",
            "Epoch 0/1: Batch 586/782: Batch Loss: 0.5125425457954407\n",
            "Epoch 0/1: Batch 587/782: Batch Loss: 0.7243989109992981\n",
            "Epoch 0/1: Batch 588/782: Batch Loss: 0.6200613379478455\n",
            "Epoch 0/1: Batch 589/782: Batch Loss: 0.7706094980239868\n",
            "Epoch 0/1: Batch 590/782: Batch Loss: 0.5592770576477051\n",
            "Epoch 0/1: Batch 591/782: Batch Loss: 0.6809703707695007\n",
            "Epoch 0/1: Batch 592/782: Batch Loss: 0.68486088514328\n",
            "Epoch 0/1: Batch 593/782: Batch Loss: 0.6701325178146362\n",
            "Epoch 0/1: Batch 594/782: Batch Loss: 0.5630876421928406\n",
            "Epoch 0/1: Batch 595/782: Batch Loss: 0.5472445487976074\n",
            "Epoch 0/1: Batch 596/782: Batch Loss: 0.7455203533172607\n",
            "Epoch 0/1: Batch 597/782: Batch Loss: 0.5765705704689026\n",
            "Epoch 0/1: Batch 598/782: Batch Loss: 0.5731793642044067\n",
            "Epoch 0/1: Batch 599/782: Batch Loss: 0.6372555494308472\n",
            "Epoch 0/1: Batch 600/782: Batch Loss: 0.624127209186554\n",
            "Epoch 0/1: Batch 601/782: Batch Loss: 0.5965608358383179\n",
            "Epoch 0/1: Batch 602/782: Batch Loss: 0.5810047388076782\n",
            "Epoch 0/1: Batch 603/782: Batch Loss: 0.5200072526931763\n",
            "Epoch 0/1: Batch 604/782: Batch Loss: 0.5332532525062561\n",
            "Epoch 0/1: Batch 605/782: Batch Loss: 0.5498049259185791\n",
            "Epoch 0/1: Batch 606/782: Batch Loss: 0.4080781042575836\n",
            "Epoch 0/1: Batch 607/782: Batch Loss: 0.5462985634803772\n",
            "Epoch 0/1: Batch 608/782: Batch Loss: 0.7715707421302795\n",
            "Epoch 0/1: Batch 609/782: Batch Loss: 0.418617308139801\n",
            "Epoch 0/1: Batch 610/782: Batch Loss: 0.555085301399231\n",
            "Epoch 0/1: Batch 611/782: Batch Loss: 0.62154221534729\n",
            "Epoch 0/1: Batch 612/782: Batch Loss: 0.2875198423862457\n",
            "Epoch 0/1: Batch 613/782: Batch Loss: 0.6816074848175049\n",
            "Epoch 0/1: Batch 614/782: Batch Loss: 0.47054925560951233\n",
            "Epoch 0/1: Batch 615/782: Batch Loss: 0.6370700597763062\n",
            "Epoch 0/1: Batch 616/782: Batch Loss: 0.8261442184448242\n",
            "Epoch 0/1: Batch 617/782: Batch Loss: 0.4155462980270386\n",
            "Epoch 0/1: Batch 618/782: Batch Loss: 0.6584951877593994\n",
            "Epoch 0/1: Batch 619/782: Batch Loss: 0.6110387444496155\n",
            "Epoch 0/1: Batch 620/782: Batch Loss: 0.5223034620285034\n",
            "Epoch 0/1: Batch 621/782: Batch Loss: 0.9133725166320801\n",
            "Epoch 0/1: Batch 622/782: Batch Loss: 0.5331110954284668\n",
            "Epoch 0/1: Batch 623/782: Batch Loss: 0.6255573630332947\n",
            "Epoch 0/1: Batch 624/782: Batch Loss: 0.7813820242881775\n",
            "Epoch 0/1: Batch 625/782: Batch Loss: 0.5963358879089355\n",
            "Epoch 0/1: Batch 626/782: Batch Loss: 0.5952010750770569\n",
            "Epoch 0/1: Batch 627/782: Batch Loss: 0.5553509593009949\n",
            "Epoch 0/1: Batch 628/782: Batch Loss: 0.6975761651992798\n",
            "Epoch 0/1: Batch 629/782: Batch Loss: 0.5621769428253174\n",
            "Epoch 0/1: Batch 630/782: Batch Loss: 0.7797528505325317\n",
            "Epoch 0/1: Batch 631/782: Batch Loss: 0.8645802140235901\n",
            "Epoch 0/1: Batch 632/782: Batch Loss: 0.6586862802505493\n",
            "Epoch 0/1: Batch 633/782: Batch Loss: 0.35096362233161926\n",
            "Epoch 0/1: Batch 634/782: Batch Loss: 0.6481994986534119\n",
            "Epoch 0/1: Batch 635/782: Batch Loss: 0.6484022736549377\n",
            "Epoch 0/1: Batch 636/782: Batch Loss: 0.5448915958404541\n",
            "Epoch 0/1: Batch 637/782: Batch Loss: 0.6016672253608704\n",
            "Epoch 0/1: Batch 638/782: Batch Loss: 0.49194610118865967\n",
            "Epoch 0/1: Batch 639/782: Batch Loss: 0.736717164516449\n",
            "Epoch 0/1: Batch 640/782: Batch Loss: 0.5623134970664978\n",
            "Epoch 0/1: Batch 641/782: Batch Loss: 0.7292582392692566\n",
            "Epoch 0/1: Batch 642/782: Batch Loss: 0.5828906297683716\n",
            "Epoch 0/1: Batch 643/782: Batch Loss: 0.4581489562988281\n",
            "Epoch 0/1: Batch 644/782: Batch Loss: 0.5814252495765686\n",
            "Epoch 0/1: Batch 645/782: Batch Loss: 0.5978867411613464\n",
            "Epoch 0/1: Batch 646/782: Batch Loss: 0.43867239356040955\n",
            "Epoch 0/1: Batch 647/782: Batch Loss: 0.5806013345718384\n",
            "Epoch 0/1: Batch 648/782: Batch Loss: 0.6225584745407104\n",
            "Epoch 0/1: Batch 649/782: Batch Loss: 0.7965388298034668\n",
            "Epoch 0/1: Batch 650/782: Batch Loss: 0.5562088489532471\n",
            "Epoch 0/1: Batch 651/782: Batch Loss: 0.6063842177391052\n",
            "Epoch 0/1: Batch 652/782: Batch Loss: 0.5559166669845581\n",
            "Epoch 0/1: Batch 653/782: Batch Loss: 0.770411491394043\n",
            "Epoch 0/1: Batch 654/782: Batch Loss: 0.6378005146980286\n",
            "Epoch 0/1: Batch 655/782: Batch Loss: 0.48261529207229614\n",
            "Epoch 0/1: Batch 656/782: Batch Loss: 0.6192395687103271\n",
            "Epoch 0/1: Batch 657/782: Batch Loss: 0.5204563736915588\n",
            "Epoch 0/1: Batch 658/782: Batch Loss: 0.4908186197280884\n",
            "Epoch 0/1: Batch 659/782: Batch Loss: 0.718054473400116\n",
            "Epoch 0/1: Batch 660/782: Batch Loss: 0.7429877519607544\n",
            "Epoch 0/1: Batch 661/782: Batch Loss: 0.5027353763580322\n",
            "Epoch 0/1: Batch 662/782: Batch Loss: 0.6221248507499695\n",
            "Epoch 0/1: Batch 663/782: Batch Loss: 0.5735520124435425\n",
            "Epoch 0/1: Batch 664/782: Batch Loss: 0.5326733589172363\n",
            "Epoch 0/1: Batch 665/782: Batch Loss: 0.5724503397941589\n",
            "Epoch 0/1: Batch 666/782: Batch Loss: 0.34783875942230225\n",
            "Epoch 0/1: Batch 667/782: Batch Loss: 0.5937185287475586\n",
            "Epoch 0/1: Batch 668/782: Batch Loss: 0.43408310413360596\n",
            "Epoch 0/1: Batch 669/782: Batch Loss: 0.5854520797729492\n",
            "Epoch 0/1: Batch 670/782: Batch Loss: 0.5392162203788757\n",
            "Epoch 0/1: Batch 671/782: Batch Loss: 0.7726607918739319\n",
            "Epoch 0/1: Batch 672/782: Batch Loss: 0.39862826466560364\n",
            "Epoch 0/1: Batch 673/782: Batch Loss: 0.6004120111465454\n",
            "Epoch 0/1: Batch 674/782: Batch Loss: 0.48125675320625305\n",
            "Epoch 0/1: Batch 675/782: Batch Loss: 0.46606895327568054\n",
            "Epoch 0/1: Batch 676/782: Batch Loss: 0.9119832515716553\n",
            "Epoch 0/1: Batch 677/782: Batch Loss: 0.4915895462036133\n",
            "Epoch 0/1: Batch 678/782: Batch Loss: 0.3241238594055176\n",
            "Epoch 0/1: Batch 679/782: Batch Loss: 0.40141770243644714\n",
            "Epoch 0/1: Batch 680/782: Batch Loss: 0.6972288489341736\n",
            "Epoch 0/1: Batch 681/782: Batch Loss: 0.7280234098434448\n",
            "Epoch 0/1: Batch 682/782: Batch Loss: 0.5409544706344604\n",
            "Epoch 0/1: Batch 683/782: Batch Loss: 0.664400577545166\n",
            "Epoch 0/1: Batch 684/782: Batch Loss: 0.7213670015335083\n",
            "Epoch 0/1: Batch 685/782: Batch Loss: 0.6721140146255493\n",
            "Epoch 0/1: Batch 686/782: Batch Loss: 0.6055339574813843\n",
            "Epoch 0/1: Batch 687/782: Batch Loss: 0.6314175724983215\n",
            "Epoch 0/1: Batch 688/782: Batch Loss: 0.540647029876709\n",
            "Epoch 0/1: Batch 689/782: Batch Loss: 0.5325639843940735\n",
            "Epoch 0/1: Batch 690/782: Batch Loss: 0.5921708345413208\n",
            "Epoch 0/1: Batch 691/782: Batch Loss: 0.4285428524017334\n",
            "Epoch 0/1: Batch 692/782: Batch Loss: 0.6167578101158142\n",
            "Epoch 0/1: Batch 693/782: Batch Loss: 0.5893868207931519\n",
            "Epoch 0/1: Batch 694/782: Batch Loss: 0.5212698578834534\n",
            "Epoch 0/1: Batch 695/782: Batch Loss: 0.8411516547203064\n",
            "Epoch 0/1: Batch 696/782: Batch Loss: 0.40667933225631714\n",
            "Epoch 0/1: Batch 697/782: Batch Loss: 0.5373751521110535\n",
            "Epoch 0/1: Batch 698/782: Batch Loss: 0.6020926833152771\n",
            "Epoch 0/1: Batch 699/782: Batch Loss: 1.1227715015411377\n",
            "Epoch 0/1: Batch 700/782: Batch Loss: 0.5060654878616333\n",
            "Epoch 0/1: Batch 701/782: Batch Loss: 0.5313076972961426\n",
            "Epoch 0/1: Batch 702/782: Batch Loss: 0.6316230297088623\n",
            "Epoch 0/1: Batch 703/782: Batch Loss: 0.628121554851532\n",
            "Epoch 0/1: Batch 704/782: Batch Loss: 0.5640110969543457\n",
            "Epoch 0/1: Batch 705/782: Batch Loss: 0.8707742094993591\n",
            "Epoch 0/1: Batch 706/782: Batch Loss: 0.7836280465126038\n",
            "Epoch 0/1: Batch 707/782: Batch Loss: 0.5357096791267395\n",
            "Epoch 0/1: Batch 708/782: Batch Loss: 0.7254178524017334\n",
            "Epoch 0/1: Batch 709/782: Batch Loss: 0.5341406464576721\n",
            "Epoch 0/1: Batch 710/782: Batch Loss: 0.7274830341339111\n",
            "Epoch 0/1: Batch 711/782: Batch Loss: 0.4702654778957367\n",
            "Epoch 0/1: Batch 712/782: Batch Loss: 0.5441836714744568\n",
            "Epoch 0/1: Batch 713/782: Batch Loss: 0.728115975856781\n",
            "Epoch 0/1: Batch 714/782: Batch Loss: 0.8306957483291626\n",
            "Epoch 0/1: Batch 715/782: Batch Loss: 0.3369780480861664\n",
            "Epoch 0/1: Batch 716/782: Batch Loss: 0.5806065797805786\n",
            "Epoch 0/1: Batch 717/782: Batch Loss: 0.45596107840538025\n",
            "Epoch 0/1: Batch 718/782: Batch Loss: 0.9077862501144409\n",
            "Epoch 0/1: Batch 719/782: Batch Loss: 0.6264066696166992\n",
            "Epoch 0/1: Batch 720/782: Batch Loss: 0.6075345873832703\n",
            "Epoch 0/1: Batch 721/782: Batch Loss: 0.44700759649276733\n",
            "Epoch 0/1: Batch 722/782: Batch Loss: 0.6068392395973206\n",
            "Epoch 0/1: Batch 723/782: Batch Loss: 0.5881590843200684\n",
            "Epoch 0/1: Batch 724/782: Batch Loss: 0.5606620907783508\n",
            "Epoch 0/1: Batch 725/782: Batch Loss: 0.6236358284950256\n",
            "Epoch 0/1: Batch 726/782: Batch Loss: 0.46426233649253845\n",
            "Epoch 0/1: Batch 727/782: Batch Loss: 0.5720794796943665\n",
            "Epoch 0/1: Batch 728/782: Batch Loss: 0.8386690616607666\n",
            "Epoch 0/1: Batch 729/782: Batch Loss: 0.6535613536834717\n",
            "Epoch 0/1: Batch 730/782: Batch Loss: 0.47100579738616943\n",
            "Epoch 0/1: Batch 731/782: Batch Loss: 0.6853178143501282\n",
            "Epoch 0/1: Batch 732/782: Batch Loss: 0.43545660376548767\n",
            "Epoch 0/1: Batch 733/782: Batch Loss: 0.572719156742096\n",
            "Epoch 0/1: Batch 734/782: Batch Loss: 0.40536004304885864\n",
            "Epoch 0/1: Batch 735/782: Batch Loss: 0.7875155806541443\n",
            "Epoch 0/1: Batch 736/782: Batch Loss: 0.5084375143051147\n",
            "Epoch 0/1: Batch 737/782: Batch Loss: 0.5327083468437195\n",
            "Epoch 0/1: Batch 738/782: Batch Loss: 0.6283392906188965\n",
            "Epoch 0/1: Batch 739/782: Batch Loss: 0.6011757850646973\n",
            "Epoch 0/1: Batch 740/782: Batch Loss: 0.5946497321128845\n",
            "Epoch 0/1: Batch 741/782: Batch Loss: 0.43466055393218994\n",
            "Epoch 0/1: Batch 742/782: Batch Loss: 0.584551215171814\n",
            "Epoch 0/1: Batch 743/782: Batch Loss: 0.944823145866394\n",
            "Epoch 0/1: Batch 744/782: Batch Loss: 0.523239016532898\n",
            "Epoch 0/1: Batch 745/782: Batch Loss: 0.3705945611000061\n",
            "Epoch 0/1: Batch 746/782: Batch Loss: 0.5552104711532593\n",
            "Epoch 0/1: Batch 747/782: Batch Loss: 0.5972949862480164\n",
            "Epoch 0/1: Batch 748/782: Batch Loss: 0.7317157983779907\n",
            "Epoch 0/1: Batch 749/782: Batch Loss: 0.5412267446517944\n",
            "Epoch 0/1: Batch 750/782: Batch Loss: 0.5355877876281738\n",
            "Epoch 0/1: Batch 751/782: Batch Loss: 0.8159260749816895\n",
            "Epoch 0/1: Batch 752/782: Batch Loss: 0.6405231356620789\n",
            "Epoch 0/1: Batch 753/782: Batch Loss: 0.7163470387458801\n",
            "Epoch 0/1: Batch 754/782: Batch Loss: 0.5819591879844666\n",
            "Epoch 0/1: Batch 755/782: Batch Loss: 0.4097396731376648\n",
            "Epoch 0/1: Batch 756/782: Batch Loss: 0.7913171648979187\n",
            "Epoch 0/1: Batch 757/782: Batch Loss: 0.3693946599960327\n",
            "Epoch 0/1: Batch 758/782: Batch Loss: 0.5481769442558289\n",
            "Epoch 0/1: Batch 759/782: Batch Loss: 0.4966314733028412\n",
            "Epoch 0/1: Batch 760/782: Batch Loss: 0.6191625595092773\n",
            "Epoch 0/1: Batch 761/782: Batch Loss: 0.6224305033683777\n",
            "Epoch 0/1: Batch 762/782: Batch Loss: 0.6056956052780151\n",
            "Epoch 0/1: Batch 763/782: Batch Loss: 0.6456231474876404\n",
            "Epoch 0/1: Batch 764/782: Batch Loss: 0.6025331020355225\n",
            "Epoch 0/1: Batch 765/782: Batch Loss: 0.4410982131958008\n",
            "Epoch 0/1: Batch 766/782: Batch Loss: 0.9304409623146057\n",
            "Epoch 0/1: Batch 767/782: Batch Loss: 0.5455011129379272\n",
            "Epoch 0/1: Batch 768/782: Batch Loss: 0.844515860080719\n",
            "Epoch 0/1: Batch 769/782: Batch Loss: 0.5057950019836426\n",
            "Epoch 0/1: Batch 770/782: Batch Loss: 0.6565341949462891\n",
            "Epoch 0/1: Batch 771/782: Batch Loss: 0.5168426632881165\n",
            "Epoch 0/1: Batch 772/782: Batch Loss: 0.7554545998573303\n",
            "Epoch 0/1: Batch 773/782: Batch Loss: 0.9183559417724609\n",
            "Epoch 0/1: Batch 774/782: Batch Loss: 0.6569890975952148\n",
            "Epoch 0/1: Batch 775/782: Batch Loss: 0.49412262439727783\n",
            "Epoch 0/1: Batch 776/782: Batch Loss: 0.6079890727996826\n",
            "Epoch 0/1: Batch 777/782: Batch Loss: 0.5224907398223877\n",
            "Epoch 0/1: Batch 778/782: Batch Loss: 0.6127907633781433\n",
            "Epoch 0/1: Batch 779/782: Batch Loss: 0.47736847400665283\n",
            "Epoch 0/1: Batch 780/782: Batch Loss: 0.5247009992599487\n",
            "Epoch 0/1: Batch 781/782: Batch Loss: 0.6388781666755676\n",
            "Epoch 0/1: Batch 782/782: Batch Loss: 0.43608418107032776\n",
            "Training Loss: 0.6516314015135436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpwKS2xceC2x",
        "colab_type": "code",
        "outputId": "302fde31-f1f9-4fc0-dd93-7921f19c8e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "num_correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for batch, (images, labels) in enumerate(testloader,1):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    logps=model(images)\n",
        "    output = torch.exp(logps)\n",
        "    pred = torch.argmax(output,1)\n",
        "    total += labels.size(0)\n",
        "    num_correct += (pred == labels).sum().item()\n",
        "    print(f'Batch {batch}/{len(testloader)}: Accuracy: {(pred == labels).sum().item()*100/labels.size(0)} %') \n",
        "  print(f'Total Accuracy after {total} images: {num_correct*100/total} %') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1/157: Accuracy: 89.0625 %\n",
            "Batch 2/157: Accuracy: 84.375 %\n",
            "Batch 3/157: Accuracy: 81.25 %\n",
            "Batch 4/157: Accuracy: 76.5625 %\n",
            "Batch 5/157: Accuracy: 78.125 %\n",
            "Batch 6/157: Accuracy: 87.5 %\n",
            "Batch 7/157: Accuracy: 79.6875 %\n",
            "Batch 8/157: Accuracy: 87.5 %\n",
            "Batch 9/157: Accuracy: 84.375 %\n",
            "Batch 10/157: Accuracy: 81.25 %\n",
            "Batch 11/157: Accuracy: 81.25 %\n",
            "Batch 12/157: Accuracy: 84.375 %\n",
            "Batch 13/157: Accuracy: 89.0625 %\n",
            "Batch 14/157: Accuracy: 79.6875 %\n",
            "Batch 15/157: Accuracy: 81.25 %\n",
            "Batch 16/157: Accuracy: 85.9375 %\n",
            "Batch 17/157: Accuracy: 85.9375 %\n",
            "Batch 18/157: Accuracy: 79.6875 %\n",
            "Batch 19/157: Accuracy: 85.9375 %\n",
            "Batch 20/157: Accuracy: 71.875 %\n",
            "Batch 21/157: Accuracy: 85.9375 %\n",
            "Batch 22/157: Accuracy: 79.6875 %\n",
            "Batch 23/157: Accuracy: 79.6875 %\n",
            "Batch 24/157: Accuracy: 81.25 %\n",
            "Batch 25/157: Accuracy: 82.8125 %\n",
            "Batch 26/157: Accuracy: 84.375 %\n",
            "Batch 27/157: Accuracy: 84.375 %\n",
            "Batch 28/157: Accuracy: 85.9375 %\n",
            "Batch 29/157: Accuracy: 87.5 %\n",
            "Batch 30/157: Accuracy: 85.9375 %\n",
            "Batch 31/157: Accuracy: 76.5625 %\n",
            "Batch 32/157: Accuracy: 85.9375 %\n",
            "Batch 33/157: Accuracy: 87.5 %\n",
            "Batch 34/157: Accuracy: 78.125 %\n",
            "Batch 35/157: Accuracy: 84.375 %\n",
            "Batch 36/157: Accuracy: 82.8125 %\n",
            "Batch 37/157: Accuracy: 82.8125 %\n",
            "Batch 38/157: Accuracy: 76.5625 %\n",
            "Batch 39/157: Accuracy: 85.9375 %\n",
            "Batch 40/157: Accuracy: 67.1875 %\n",
            "Batch 41/157: Accuracy: 81.25 %\n",
            "Batch 42/157: Accuracy: 81.25 %\n",
            "Batch 43/157: Accuracy: 90.625 %\n",
            "Batch 44/157: Accuracy: 79.6875 %\n",
            "Batch 45/157: Accuracy: 85.9375 %\n",
            "Batch 46/157: Accuracy: 84.375 %\n",
            "Batch 47/157: Accuracy: 85.9375 %\n",
            "Batch 48/157: Accuracy: 78.125 %\n",
            "Batch 49/157: Accuracy: 85.9375 %\n",
            "Batch 50/157: Accuracy: 89.0625 %\n",
            "Batch 51/157: Accuracy: 81.25 %\n",
            "Batch 52/157: Accuracy: 85.9375 %\n",
            "Batch 53/157: Accuracy: 81.25 %\n",
            "Batch 54/157: Accuracy: 81.25 %\n",
            "Batch 55/157: Accuracy: 84.375 %\n",
            "Batch 56/157: Accuracy: 82.8125 %\n",
            "Batch 57/157: Accuracy: 84.375 %\n",
            "Batch 58/157: Accuracy: 84.375 %\n",
            "Batch 59/157: Accuracy: 75.0 %\n",
            "Batch 60/157: Accuracy: 78.125 %\n",
            "Batch 61/157: Accuracy: 85.9375 %\n",
            "Batch 62/157: Accuracy: 84.375 %\n",
            "Batch 63/157: Accuracy: 87.5 %\n",
            "Batch 64/157: Accuracy: 84.375 %\n",
            "Batch 65/157: Accuracy: 89.0625 %\n",
            "Batch 66/157: Accuracy: 89.0625 %\n",
            "Batch 67/157: Accuracy: 76.5625 %\n",
            "Batch 68/157: Accuracy: 89.0625 %\n",
            "Batch 69/157: Accuracy: 87.5 %\n",
            "Batch 70/157: Accuracy: 89.0625 %\n",
            "Batch 71/157: Accuracy: 84.375 %\n",
            "Batch 72/157: Accuracy: 79.6875 %\n",
            "Batch 73/157: Accuracy: 84.375 %\n",
            "Batch 74/157: Accuracy: 81.25 %\n",
            "Batch 75/157: Accuracy: 81.25 %\n",
            "Batch 76/157: Accuracy: 87.5 %\n",
            "Batch 77/157: Accuracy: 85.9375 %\n",
            "Batch 78/157: Accuracy: 78.125 %\n",
            "Batch 79/157: Accuracy: 81.25 %\n",
            "Batch 80/157: Accuracy: 73.4375 %\n",
            "Batch 81/157: Accuracy: 85.9375 %\n",
            "Batch 82/157: Accuracy: 75.0 %\n",
            "Batch 83/157: Accuracy: 84.375 %\n",
            "Batch 84/157: Accuracy: 81.25 %\n",
            "Batch 85/157: Accuracy: 81.25 %\n",
            "Batch 86/157: Accuracy: 79.6875 %\n",
            "Batch 87/157: Accuracy: 76.5625 %\n",
            "Batch 88/157: Accuracy: 84.375 %\n",
            "Batch 89/157: Accuracy: 75.0 %\n",
            "Batch 90/157: Accuracy: 73.4375 %\n",
            "Batch 91/157: Accuracy: 81.25 %\n",
            "Batch 92/157: Accuracy: 76.5625 %\n",
            "Batch 93/157: Accuracy: 84.375 %\n",
            "Batch 94/157: Accuracy: 82.8125 %\n",
            "Batch 95/157: Accuracy: 78.125 %\n",
            "Batch 96/157: Accuracy: 85.9375 %\n",
            "Batch 97/157: Accuracy: 78.125 %\n",
            "Batch 98/157: Accuracy: 82.8125 %\n",
            "Batch 99/157: Accuracy: 90.625 %\n",
            "Batch 100/157: Accuracy: 87.5 %\n",
            "Batch 101/157: Accuracy: 81.25 %\n",
            "Batch 102/157: Accuracy: 82.8125 %\n",
            "Batch 103/157: Accuracy: 78.125 %\n",
            "Batch 104/157: Accuracy: 89.0625 %\n",
            "Batch 105/157: Accuracy: 85.9375 %\n",
            "Batch 106/157: Accuracy: 84.375 %\n",
            "Batch 107/157: Accuracy: 90.625 %\n",
            "Batch 108/157: Accuracy: 81.25 %\n",
            "Batch 109/157: Accuracy: 78.125 %\n",
            "Batch 110/157: Accuracy: 76.5625 %\n",
            "Batch 111/157: Accuracy: 78.125 %\n",
            "Batch 112/157: Accuracy: 87.5 %\n",
            "Batch 113/157: Accuracy: 84.375 %\n",
            "Batch 114/157: Accuracy: 87.5 %\n",
            "Batch 115/157: Accuracy: 93.75 %\n",
            "Batch 116/157: Accuracy: 84.375 %\n",
            "Batch 117/157: Accuracy: 84.375 %\n",
            "Batch 118/157: Accuracy: 79.6875 %\n",
            "Batch 119/157: Accuracy: 89.0625 %\n",
            "Batch 120/157: Accuracy: 90.625 %\n",
            "Batch 121/157: Accuracy: 81.25 %\n",
            "Batch 122/157: Accuracy: 84.375 %\n",
            "Batch 123/157: Accuracy: 82.8125 %\n",
            "Batch 124/157: Accuracy: 84.375 %\n",
            "Batch 125/157: Accuracy: 75.0 %\n",
            "Batch 126/157: Accuracy: 89.0625 %\n",
            "Batch 127/157: Accuracy: 87.5 %\n",
            "Batch 128/157: Accuracy: 90.625 %\n",
            "Batch 129/157: Accuracy: 81.25 %\n",
            "Batch 130/157: Accuracy: 79.6875 %\n",
            "Batch 131/157: Accuracy: 84.375 %\n",
            "Batch 132/157: Accuracy: 79.6875 %\n",
            "Batch 133/157: Accuracy: 79.6875 %\n",
            "Batch 134/157: Accuracy: 81.25 %\n",
            "Batch 135/157: Accuracy: 81.25 %\n",
            "Batch 136/157: Accuracy: 82.8125 %\n",
            "Batch 137/157: Accuracy: 70.3125 %\n",
            "Batch 138/157: Accuracy: 68.75 %\n",
            "Batch 139/157: Accuracy: 85.9375 %\n",
            "Batch 140/157: Accuracy: 84.375 %\n",
            "Batch 141/157: Accuracy: 85.9375 %\n",
            "Batch 142/157: Accuracy: 85.9375 %\n",
            "Batch 143/157: Accuracy: 79.6875 %\n",
            "Batch 144/157: Accuracy: 90.625 %\n",
            "Batch 145/157: Accuracy: 78.125 %\n",
            "Batch 146/157: Accuracy: 81.25 %\n",
            "Batch 147/157: Accuracy: 82.8125 %\n",
            "Batch 148/157: Accuracy: 87.5 %\n",
            "Batch 149/157: Accuracy: 82.8125 %\n",
            "Batch 150/157: Accuracy: 89.0625 %\n",
            "Batch 151/157: Accuracy: 90.625 %\n",
            "Batch 152/157: Accuracy: 87.5 %\n",
            "Batch 153/157: Accuracy: 73.4375 %\n",
            "Batch 154/157: Accuracy: 79.6875 %\n",
            "Batch 155/157: Accuracy: 84.375 %\n",
            "Batch 156/157: Accuracy: 84.375 %\n",
            "Batch 157/157: Accuracy: 75.0 %\n",
            "Total Accuracy after 10000 images: 82.88 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ZvfLfMj61b",
        "colab_type": "code",
        "outputId": "410ad3fb-037f-4ce9-de44-569f52512b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=10, bias=True)\n",
              "      (1): LogSoftmax()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwtATB61jtT4",
        "colab_type": "code",
        "outputId": "c51aa88c-8550-4947-9773-8425651fc050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for params in model.classifier.parameters():\n",
        "  print(params.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clg53bwMkzj9",
        "colab_type": "code",
        "outputId": "6c1db952-dde2-4e57-aa8d-c4658936397b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.classifier[6].parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Module.parameters at 0x7fc13286dca8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98z2jJTdg_Rg",
        "colab_type": "code",
        "outputId": "5d465710-ff2e-42e7-d11b-e25b4f786839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.train()\n",
        "for i in range(17,31):\n",
        "  model.features[i].requires_grad = True\n",
        "for i in range(0,7):\n",
        "  model.classifier[i].requires_grad = True\n",
        "model.classifier[6] = nn.Sequential(\n",
        "                                    nn.Linear(in_features=4096, out_features=512),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(p=0.5),\n",
        "                                    nn.Linear(in_features=512, out_features=10),\n",
        "                                    nn.LogSoftmax(dim=1))\n",
        "lr = 3e-4\n",
        "optimizer = optim.Adam([\n",
        "                        {'params': model.features[17].parameters(),'lr':lr/9},\n",
        "                        {'params': model.features[19].parameters(),'lr':lr/9},\n",
        "                        {'params': model.features[21].parameters(),'lr':lr/9},\n",
        "                        {'params': model.features[24].parameters(),'lr':lr/3},\n",
        "                        {'params': model.features[26].parameters(),'lr':lr/3},\n",
        "                        {'params': model.features[28].parameters(),'lr':lr/3},\n",
        "                        {'params': model.classifier[0].parameters(),'lr':lr},\n",
        "                        {'params': model.classifier[3].parameters(),'lr':lr},\n",
        "                        {'params': model.classifier[6].parameters(),'lr':lr}\n",
        "                        ],lr=lr)\n",
        "model.to(device)\n",
        "num_epochs = 1\n",
        "batch_loss = 0\n",
        "cum_loss = 0\n",
        "for e in range(num_epochs):\n",
        "  for batch, (images,labels) in enumerate(trainloader,1):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logps=model(images)\n",
        "    loss=criterion(logps,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_loss += loss.item()\n",
        "    print(f'Epoch {e}/{num_epochs}: Batch {batch}/{len(trainloader)}: Batch Loss: {loss.item()}')\n",
        "print(f'Training Loss: {batch_loss/len(trainloader)}')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "num_correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for batch, (images, labels) in enumerate(testloader,1):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    logps=model(images)\n",
        "    output = torch.exp(logps)\n",
        "    pred = torch.argmax(output,1)\n",
        "    total += labels.size(0)\n",
        "    num_correct += (pred == labels).sum().item()\n",
        "    print(f'Batch {batch}/{len(testloader)}: Accuracy: {(pred == labels).sum().item()*100/labels.size(0)} %') \n",
        "  print(f'Total Accuracy after {total} images: {num_correct*100/total} %') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Batch 1/782: Batch Loss: 2.311833143234253\n",
            "Epoch 0/1: Batch 2/782: Batch Loss: 2.301222562789917\n",
            "Epoch 0/1: Batch 3/782: Batch Loss: 2.2607133388519287\n",
            "Epoch 0/1: Batch 4/782: Batch Loss: 2.1801724433898926\n",
            "Epoch 0/1: Batch 5/782: Batch Loss: 2.0809450149536133\n",
            "Epoch 0/1: Batch 6/782: Batch Loss: 1.9618258476257324\n",
            "Epoch 0/1: Batch 7/782: Batch Loss: 1.9490665197372437\n",
            "Epoch 0/1: Batch 8/782: Batch Loss: 2.0000247955322266\n",
            "Epoch 0/1: Batch 9/782: Batch Loss: 1.7094556093215942\n",
            "Epoch 0/1: Batch 10/782: Batch Loss: 1.7017565965652466\n",
            "Epoch 0/1: Batch 11/782: Batch Loss: 1.6280021667480469\n",
            "Epoch 0/1: Batch 12/782: Batch Loss: 1.5916240215301514\n",
            "Epoch 0/1: Batch 13/782: Batch Loss: 1.605338454246521\n",
            "Epoch 0/1: Batch 14/782: Batch Loss: 1.5252914428710938\n",
            "Epoch 0/1: Batch 15/782: Batch Loss: 1.4200986623764038\n",
            "Epoch 0/1: Batch 16/782: Batch Loss: 1.4461557865142822\n",
            "Epoch 0/1: Batch 17/782: Batch Loss: 1.2669155597686768\n",
            "Epoch 0/1: Batch 18/782: Batch Loss: 1.1598155498504639\n",
            "Epoch 0/1: Batch 19/782: Batch Loss: 1.2496718168258667\n",
            "Epoch 0/1: Batch 20/782: Batch Loss: 1.174271583557129\n",
            "Epoch 0/1: Batch 21/782: Batch Loss: 1.2276456356048584\n",
            "Epoch 0/1: Batch 22/782: Batch Loss: 1.2710721492767334\n",
            "Epoch 0/1: Batch 23/782: Batch Loss: 1.2421828508377075\n",
            "Epoch 0/1: Batch 24/782: Batch Loss: 0.8308952450752258\n",
            "Epoch 0/1: Batch 25/782: Batch Loss: 0.9550663232803345\n",
            "Epoch 0/1: Batch 26/782: Batch Loss: 0.9903537034988403\n",
            "Epoch 0/1: Batch 27/782: Batch Loss: 1.0668879747390747\n",
            "Epoch 0/1: Batch 28/782: Batch Loss: 0.9619178175926208\n",
            "Epoch 0/1: Batch 29/782: Batch Loss: 0.8872877955436707\n",
            "Epoch 0/1: Batch 30/782: Batch Loss: 0.8129121661186218\n",
            "Epoch 0/1: Batch 31/782: Batch Loss: 1.0359373092651367\n",
            "Epoch 0/1: Batch 32/782: Batch Loss: 0.9779259562492371\n",
            "Epoch 0/1: Batch 33/782: Batch Loss: 1.1428697109222412\n",
            "Epoch 0/1: Batch 34/782: Batch Loss: 1.0325311422348022\n",
            "Epoch 0/1: Batch 35/782: Batch Loss: 0.9111068844795227\n",
            "Epoch 0/1: Batch 36/782: Batch Loss: 0.8667793273925781\n",
            "Epoch 0/1: Batch 37/782: Batch Loss: 0.7449602484703064\n",
            "Epoch 0/1: Batch 38/782: Batch Loss: 0.7085267901420593\n",
            "Epoch 0/1: Batch 39/782: Batch Loss: 0.9487329721450806\n",
            "Epoch 0/1: Batch 40/782: Batch Loss: 0.910061240196228\n",
            "Epoch 0/1: Batch 41/782: Batch Loss: 0.9345077872276306\n",
            "Epoch 0/1: Batch 42/782: Batch Loss: 0.8290353417396545\n",
            "Epoch 0/1: Batch 43/782: Batch Loss: 0.8228310346603394\n",
            "Epoch 0/1: Batch 44/782: Batch Loss: 1.0490963459014893\n",
            "Epoch 0/1: Batch 45/782: Batch Loss: 0.9729586243629456\n",
            "Epoch 0/1: Batch 46/782: Batch Loss: 0.7078418731689453\n",
            "Epoch 0/1: Batch 47/782: Batch Loss: 0.7629032731056213\n",
            "Epoch 0/1: Batch 48/782: Batch Loss: 0.9931817650794983\n",
            "Epoch 0/1: Batch 49/782: Batch Loss: 0.8554560542106628\n",
            "Epoch 0/1: Batch 50/782: Batch Loss: 0.8237136602401733\n",
            "Epoch 0/1: Batch 51/782: Batch Loss: 0.6451680660247803\n",
            "Epoch 0/1: Batch 52/782: Batch Loss: 0.9080666303634644\n",
            "Epoch 0/1: Batch 53/782: Batch Loss: 0.8214633464813232\n",
            "Epoch 0/1: Batch 54/782: Batch Loss: 0.7902959585189819\n",
            "Epoch 0/1: Batch 55/782: Batch Loss: 0.7405372262001038\n",
            "Epoch 0/1: Batch 56/782: Batch Loss: 0.639124870300293\n",
            "Epoch 0/1: Batch 57/782: Batch Loss: 0.7321054935455322\n",
            "Epoch 0/1: Batch 58/782: Batch Loss: 0.5174184441566467\n",
            "Epoch 0/1: Batch 59/782: Batch Loss: 0.7859413027763367\n",
            "Epoch 0/1: Batch 60/782: Batch Loss: 0.8159166574478149\n",
            "Epoch 0/1: Batch 61/782: Batch Loss: 0.7774894833564758\n",
            "Epoch 0/1: Batch 62/782: Batch Loss: 0.8681027889251709\n",
            "Epoch 0/1: Batch 63/782: Batch Loss: 0.7730171084403992\n",
            "Epoch 0/1: Batch 64/782: Batch Loss: 0.5575419664382935\n",
            "Epoch 0/1: Batch 65/782: Batch Loss: 0.5442231893539429\n",
            "Epoch 0/1: Batch 66/782: Batch Loss: 0.7569721341133118\n",
            "Epoch 0/1: Batch 67/782: Batch Loss: 0.7405300140380859\n",
            "Epoch 0/1: Batch 68/782: Batch Loss: 0.9056484699249268\n",
            "Epoch 0/1: Batch 69/782: Batch Loss: 0.7948942184448242\n",
            "Epoch 0/1: Batch 70/782: Batch Loss: 0.800093412399292\n",
            "Epoch 0/1: Batch 71/782: Batch Loss: 0.6943206787109375\n",
            "Epoch 0/1: Batch 72/782: Batch Loss: 0.8438530564308167\n",
            "Epoch 0/1: Batch 73/782: Batch Loss: 0.7145370841026306\n",
            "Epoch 0/1: Batch 74/782: Batch Loss: 0.7810132503509521\n",
            "Epoch 0/1: Batch 75/782: Batch Loss: 0.931373655796051\n",
            "Epoch 0/1: Batch 76/782: Batch Loss: 0.6347919702529907\n",
            "Epoch 0/1: Batch 77/782: Batch Loss: 0.8531200885772705\n",
            "Epoch 0/1: Batch 78/782: Batch Loss: 0.8963021636009216\n",
            "Epoch 0/1: Batch 79/782: Batch Loss: 0.879099428653717\n",
            "Epoch 0/1: Batch 80/782: Batch Loss: 0.7503079771995544\n",
            "Epoch 0/1: Batch 81/782: Batch Loss: 0.6545157432556152\n",
            "Epoch 0/1: Batch 82/782: Batch Loss: 0.9500224590301514\n",
            "Epoch 0/1: Batch 83/782: Batch Loss: 0.8001291155815125\n",
            "Epoch 0/1: Batch 84/782: Batch Loss: 0.7497692704200745\n",
            "Epoch 0/1: Batch 85/782: Batch Loss: 0.7240302562713623\n",
            "Epoch 0/1: Batch 86/782: Batch Loss: 0.779765248298645\n",
            "Epoch 0/1: Batch 87/782: Batch Loss: 0.6648457050323486\n",
            "Epoch 0/1: Batch 88/782: Batch Loss: 0.6054249405860901\n",
            "Epoch 0/1: Batch 89/782: Batch Loss: 0.6784735918045044\n",
            "Epoch 0/1: Batch 90/782: Batch Loss: 0.7490020394325256\n",
            "Epoch 0/1: Batch 91/782: Batch Loss: 0.6101797819137573\n",
            "Epoch 0/1: Batch 92/782: Batch Loss: 0.8596997261047363\n",
            "Epoch 0/1: Batch 93/782: Batch Loss: 0.8667198419570923\n",
            "Epoch 0/1: Batch 94/782: Batch Loss: 0.697160005569458\n",
            "Epoch 0/1: Batch 95/782: Batch Loss: 0.7331323027610779\n",
            "Epoch 0/1: Batch 96/782: Batch Loss: 0.8688503503799438\n",
            "Epoch 0/1: Batch 97/782: Batch Loss: 0.6483850479125977\n",
            "Epoch 0/1: Batch 98/782: Batch Loss: 0.6197255253791809\n",
            "Epoch 0/1: Batch 99/782: Batch Loss: 0.6115273833274841\n",
            "Epoch 0/1: Batch 100/782: Batch Loss: 0.777916669845581\n",
            "Epoch 0/1: Batch 101/782: Batch Loss: 0.7425556182861328\n",
            "Epoch 0/1: Batch 102/782: Batch Loss: 0.42085981369018555\n",
            "Epoch 0/1: Batch 103/782: Batch Loss: 0.7029916048049927\n",
            "Epoch 0/1: Batch 104/782: Batch Loss: 0.6535574197769165\n",
            "Epoch 0/1: Batch 105/782: Batch Loss: 0.6995545625686646\n",
            "Epoch 0/1: Batch 106/782: Batch Loss: 0.6273459792137146\n",
            "Epoch 0/1: Batch 107/782: Batch Loss: 0.8371806144714355\n",
            "Epoch 0/1: Batch 108/782: Batch Loss: 0.7307608723640442\n",
            "Epoch 0/1: Batch 109/782: Batch Loss: 0.8380962014198303\n",
            "Epoch 0/1: Batch 110/782: Batch Loss: 0.7672241926193237\n",
            "Epoch 0/1: Batch 111/782: Batch Loss: 0.6738876104354858\n",
            "Epoch 0/1: Batch 112/782: Batch Loss: 0.6101474761962891\n",
            "Epoch 0/1: Batch 113/782: Batch Loss: 0.6934322118759155\n",
            "Epoch 0/1: Batch 114/782: Batch Loss: 0.6414039134979248\n",
            "Epoch 0/1: Batch 115/782: Batch Loss: 0.6728395223617554\n",
            "Epoch 0/1: Batch 116/782: Batch Loss: 0.5338800549507141\n",
            "Epoch 0/1: Batch 117/782: Batch Loss: 0.5877594947814941\n",
            "Epoch 0/1: Batch 118/782: Batch Loss: 0.7075433135032654\n",
            "Epoch 0/1: Batch 119/782: Batch Loss: 0.6980834007263184\n",
            "Epoch 0/1: Batch 120/782: Batch Loss: 0.7133640050888062\n",
            "Epoch 0/1: Batch 121/782: Batch Loss: 0.8123853206634521\n",
            "Epoch 0/1: Batch 122/782: Batch Loss: 0.9048904180526733\n",
            "Epoch 0/1: Batch 123/782: Batch Loss: 0.7476215362548828\n",
            "Epoch 0/1: Batch 124/782: Batch Loss: 0.6109195947647095\n",
            "Epoch 0/1: Batch 125/782: Batch Loss: 0.5539460182189941\n",
            "Epoch 0/1: Batch 126/782: Batch Loss: 0.6006910800933838\n",
            "Epoch 0/1: Batch 127/782: Batch Loss: 0.680471658706665\n",
            "Epoch 0/1: Batch 128/782: Batch Loss: 0.811691164970398\n",
            "Epoch 0/1: Batch 129/782: Batch Loss: 0.6707761287689209\n",
            "Epoch 0/1: Batch 130/782: Batch Loss: 0.589535117149353\n",
            "Epoch 0/1: Batch 131/782: Batch Loss: 0.7237730622291565\n",
            "Epoch 0/1: Batch 132/782: Batch Loss: 0.8367197513580322\n",
            "Epoch 0/1: Batch 133/782: Batch Loss: 0.6832533478736877\n",
            "Epoch 0/1: Batch 134/782: Batch Loss: 0.6298179030418396\n",
            "Epoch 0/1: Batch 135/782: Batch Loss: 0.5843629240989685\n",
            "Epoch 0/1: Batch 136/782: Batch Loss: 0.6684868931770325\n",
            "Epoch 0/1: Batch 137/782: Batch Loss: 0.6664144992828369\n",
            "Epoch 0/1: Batch 138/782: Batch Loss: 0.740389347076416\n",
            "Epoch 0/1: Batch 139/782: Batch Loss: 0.830453097820282\n",
            "Epoch 0/1: Batch 140/782: Batch Loss: 0.6751691699028015\n",
            "Epoch 0/1: Batch 141/782: Batch Loss: 0.6559457778930664\n",
            "Epoch 0/1: Batch 142/782: Batch Loss: 0.6095888614654541\n",
            "Epoch 0/1: Batch 143/782: Batch Loss: 0.887910008430481\n",
            "Epoch 0/1: Batch 144/782: Batch Loss: 0.8772889971733093\n",
            "Epoch 0/1: Batch 145/782: Batch Loss: 0.6704690456390381\n",
            "Epoch 0/1: Batch 146/782: Batch Loss: 0.7382122278213501\n",
            "Epoch 0/1: Batch 147/782: Batch Loss: 0.5484573245048523\n",
            "Epoch 0/1: Batch 148/782: Batch Loss: 0.6596085429191589\n",
            "Epoch 0/1: Batch 149/782: Batch Loss: 0.5904965996742249\n",
            "Epoch 0/1: Batch 150/782: Batch Loss: 0.6198458671569824\n",
            "Epoch 0/1: Batch 151/782: Batch Loss: 0.733594536781311\n",
            "Epoch 0/1: Batch 152/782: Batch Loss: 0.6569240689277649\n",
            "Epoch 0/1: Batch 153/782: Batch Loss: 0.6212619543075562\n",
            "Epoch 0/1: Batch 154/782: Batch Loss: 0.7426919341087341\n",
            "Epoch 0/1: Batch 155/782: Batch Loss: 0.8214675784111023\n",
            "Epoch 0/1: Batch 156/782: Batch Loss: 0.7196418642997742\n",
            "Epoch 0/1: Batch 157/782: Batch Loss: 0.48475727438926697\n",
            "Epoch 0/1: Batch 158/782: Batch Loss: 0.688324511051178\n",
            "Epoch 0/1: Batch 159/782: Batch Loss: 0.58130943775177\n",
            "Epoch 0/1: Batch 160/782: Batch Loss: 0.7061353921890259\n",
            "Epoch 0/1: Batch 161/782: Batch Loss: 0.6058410406112671\n",
            "Epoch 0/1: Batch 162/782: Batch Loss: 0.679719090461731\n",
            "Epoch 0/1: Batch 163/782: Batch Loss: 0.6576210856437683\n",
            "Epoch 0/1: Batch 164/782: Batch Loss: 0.651725709438324\n",
            "Epoch 0/1: Batch 165/782: Batch Loss: 0.7102158069610596\n",
            "Epoch 0/1: Batch 166/782: Batch Loss: 0.773200511932373\n",
            "Epoch 0/1: Batch 167/782: Batch Loss: 0.967415988445282\n",
            "Epoch 0/1: Batch 168/782: Batch Loss: 0.6902593970298767\n",
            "Epoch 0/1: Batch 169/782: Batch Loss: 0.6351945996284485\n",
            "Epoch 0/1: Batch 170/782: Batch Loss: 0.7463473081588745\n",
            "Epoch 0/1: Batch 171/782: Batch Loss: 0.9545722007751465\n",
            "Epoch 0/1: Batch 172/782: Batch Loss: 0.6730011701583862\n",
            "Epoch 0/1: Batch 173/782: Batch Loss: 0.7685118913650513\n",
            "Epoch 0/1: Batch 174/782: Batch Loss: 0.6371492743492126\n",
            "Epoch 0/1: Batch 175/782: Batch Loss: 0.7851992249488831\n",
            "Epoch 0/1: Batch 176/782: Batch Loss: 0.5518258213996887\n",
            "Epoch 0/1: Batch 177/782: Batch Loss: 0.6480371356010437\n",
            "Epoch 0/1: Batch 178/782: Batch Loss: 0.6714680790901184\n",
            "Epoch 0/1: Batch 179/782: Batch Loss: 0.6382610201835632\n",
            "Epoch 0/1: Batch 180/782: Batch Loss: 0.6968392133712769\n",
            "Epoch 0/1: Batch 181/782: Batch Loss: 0.7393196821212769\n",
            "Epoch 0/1: Batch 182/782: Batch Loss: 0.8300527930259705\n",
            "Epoch 0/1: Batch 183/782: Batch Loss: 0.7841619253158569\n",
            "Epoch 0/1: Batch 184/782: Batch Loss: 0.6753105521202087\n",
            "Epoch 0/1: Batch 185/782: Batch Loss: 0.7490003108978271\n",
            "Epoch 0/1: Batch 186/782: Batch Loss: 0.8194732069969177\n",
            "Epoch 0/1: Batch 187/782: Batch Loss: 0.9517532587051392\n",
            "Epoch 0/1: Batch 188/782: Batch Loss: 0.5265169739723206\n",
            "Epoch 0/1: Batch 189/782: Batch Loss: 1.0034123659133911\n",
            "Epoch 0/1: Batch 190/782: Batch Loss: 0.7732264995574951\n",
            "Epoch 0/1: Batch 191/782: Batch Loss: 0.6863252520561218\n",
            "Epoch 0/1: Batch 192/782: Batch Loss: 0.6394426822662354\n",
            "Epoch 0/1: Batch 193/782: Batch Loss: 0.6307845711708069\n",
            "Epoch 0/1: Batch 194/782: Batch Loss: 0.8723705410957336\n",
            "Epoch 0/1: Batch 195/782: Batch Loss: 0.776978075504303\n",
            "Epoch 0/1: Batch 196/782: Batch Loss: 0.619139552116394\n",
            "Epoch 0/1: Batch 197/782: Batch Loss: 0.6220145225524902\n",
            "Epoch 0/1: Batch 198/782: Batch Loss: 0.8210179209709167\n",
            "Epoch 0/1: Batch 199/782: Batch Loss: 0.7240025997161865\n",
            "Epoch 0/1: Batch 200/782: Batch Loss: 0.5851216912269592\n",
            "Epoch 0/1: Batch 201/782: Batch Loss: 0.9396037459373474\n",
            "Epoch 0/1: Batch 202/782: Batch Loss: 0.7561888694763184\n",
            "Epoch 0/1: Batch 203/782: Batch Loss: 0.729317307472229\n",
            "Epoch 0/1: Batch 204/782: Batch Loss: 0.5528389811515808\n",
            "Epoch 0/1: Batch 205/782: Batch Loss: 0.5582819581031799\n",
            "Epoch 0/1: Batch 206/782: Batch Loss: 0.57119220495224\n",
            "Epoch 0/1: Batch 207/782: Batch Loss: 0.838850736618042\n",
            "Epoch 0/1: Batch 208/782: Batch Loss: 0.44354215264320374\n",
            "Epoch 0/1: Batch 209/782: Batch Loss: 0.8897718787193298\n",
            "Epoch 0/1: Batch 210/782: Batch Loss: 0.6402333974838257\n",
            "Epoch 0/1: Batch 211/782: Batch Loss: 0.5730001330375671\n",
            "Epoch 0/1: Batch 212/782: Batch Loss: 0.827987015247345\n",
            "Epoch 0/1: Batch 213/782: Batch Loss: 0.7093620300292969\n",
            "Epoch 0/1: Batch 214/782: Batch Loss: 0.7337259650230408\n",
            "Epoch 0/1: Batch 215/782: Batch Loss: 0.4784882068634033\n",
            "Epoch 0/1: Batch 216/782: Batch Loss: 0.5338512659072876\n",
            "Epoch 0/1: Batch 217/782: Batch Loss: 0.8213163614273071\n",
            "Epoch 0/1: Batch 218/782: Batch Loss: 0.7133000493049622\n",
            "Epoch 0/1: Batch 219/782: Batch Loss: 0.9919112920761108\n",
            "Epoch 0/1: Batch 220/782: Batch Loss: 0.5585765242576599\n",
            "Epoch 0/1: Batch 221/782: Batch Loss: 0.6828856468200684\n",
            "Epoch 0/1: Batch 222/782: Batch Loss: 0.5714519619941711\n",
            "Epoch 0/1: Batch 223/782: Batch Loss: 0.6318830847740173\n",
            "Epoch 0/1: Batch 224/782: Batch Loss: 0.8476952910423279\n",
            "Epoch 0/1: Batch 225/782: Batch Loss: 0.7323432564735413\n",
            "Epoch 0/1: Batch 226/782: Batch Loss: 0.8311119079589844\n",
            "Epoch 0/1: Batch 227/782: Batch Loss: 0.8143991231918335\n",
            "Epoch 0/1: Batch 228/782: Batch Loss: 0.7150641679763794\n",
            "Epoch 0/1: Batch 229/782: Batch Loss: 0.8903632760047913\n",
            "Epoch 0/1: Batch 230/782: Batch Loss: 0.8167262077331543\n",
            "Epoch 0/1: Batch 231/782: Batch Loss: 0.48982909321784973\n",
            "Epoch 0/1: Batch 232/782: Batch Loss: 0.6115477681159973\n",
            "Epoch 0/1: Batch 233/782: Batch Loss: 0.5941768884658813\n",
            "Epoch 0/1: Batch 234/782: Batch Loss: 0.5999681353569031\n",
            "Epoch 0/1: Batch 235/782: Batch Loss: 0.5982154607772827\n",
            "Epoch 0/1: Batch 236/782: Batch Loss: 0.7675784230232239\n",
            "Epoch 0/1: Batch 237/782: Batch Loss: 0.7945714592933655\n",
            "Epoch 0/1: Batch 238/782: Batch Loss: 0.7986592650413513\n",
            "Epoch 0/1: Batch 239/782: Batch Loss: 0.721912145614624\n",
            "Epoch 0/1: Batch 240/782: Batch Loss: 0.6850467324256897\n",
            "Epoch 0/1: Batch 241/782: Batch Loss: 0.8019488453865051\n",
            "Epoch 0/1: Batch 242/782: Batch Loss: 0.5454927682876587\n",
            "Epoch 0/1: Batch 243/782: Batch Loss: 0.7894253134727478\n",
            "Epoch 0/1: Batch 244/782: Batch Loss: 0.5465772151947021\n",
            "Epoch 0/1: Batch 245/782: Batch Loss: 0.6996347904205322\n",
            "Epoch 0/1: Batch 246/782: Batch Loss: 0.6798325777053833\n",
            "Epoch 0/1: Batch 247/782: Batch Loss: 0.6032502055168152\n",
            "Epoch 0/1: Batch 248/782: Batch Loss: 0.7907087802886963\n",
            "Epoch 0/1: Batch 249/782: Batch Loss: 0.520758867263794\n",
            "Epoch 0/1: Batch 250/782: Batch Loss: 0.6502872109413147\n",
            "Epoch 0/1: Batch 251/782: Batch Loss: 0.5706048607826233\n",
            "Epoch 0/1: Batch 252/782: Batch Loss: 0.660382866859436\n",
            "Epoch 0/1: Batch 253/782: Batch Loss: 0.5829793214797974\n",
            "Epoch 0/1: Batch 254/782: Batch Loss: 0.6278092861175537\n",
            "Epoch 0/1: Batch 255/782: Batch Loss: 0.562130331993103\n",
            "Epoch 0/1: Batch 256/782: Batch Loss: 0.5252698063850403\n",
            "Epoch 0/1: Batch 257/782: Batch Loss: 0.6614128947257996\n",
            "Epoch 0/1: Batch 258/782: Batch Loss: 0.7604284286499023\n",
            "Epoch 0/1: Batch 259/782: Batch Loss: 0.7935454249382019\n",
            "Epoch 0/1: Batch 260/782: Batch Loss: 0.6159853935241699\n",
            "Epoch 0/1: Batch 261/782: Batch Loss: 0.5760361552238464\n",
            "Epoch 0/1: Batch 262/782: Batch Loss: 0.42387381196022034\n",
            "Epoch 0/1: Batch 263/782: Batch Loss: 0.5579035878181458\n",
            "Epoch 0/1: Batch 264/782: Batch Loss: 0.8021419048309326\n",
            "Epoch 0/1: Batch 265/782: Batch Loss: 0.5457907915115356\n",
            "Epoch 0/1: Batch 266/782: Batch Loss: 0.5123459100723267\n",
            "Epoch 0/1: Batch 267/782: Batch Loss: 0.7328541278839111\n",
            "Epoch 0/1: Batch 268/782: Batch Loss: 0.7223663330078125\n",
            "Epoch 0/1: Batch 269/782: Batch Loss: 0.5086725354194641\n",
            "Epoch 0/1: Batch 270/782: Batch Loss: 0.7935587167739868\n",
            "Epoch 0/1: Batch 271/782: Batch Loss: 0.48109978437423706\n",
            "Epoch 0/1: Batch 272/782: Batch Loss: 0.6972066164016724\n",
            "Epoch 0/1: Batch 273/782: Batch Loss: 0.914874255657196\n",
            "Epoch 0/1: Batch 274/782: Batch Loss: 0.7179467678070068\n",
            "Epoch 0/1: Batch 275/782: Batch Loss: 0.7271231412887573\n",
            "Epoch 0/1: Batch 276/782: Batch Loss: 0.5646529793739319\n",
            "Epoch 0/1: Batch 277/782: Batch Loss: 0.6231716871261597\n",
            "Epoch 0/1: Batch 278/782: Batch Loss: 0.6756965517997742\n",
            "Epoch 0/1: Batch 279/782: Batch Loss: 0.6443297266960144\n",
            "Epoch 0/1: Batch 280/782: Batch Loss: 0.7026677131652832\n",
            "Epoch 0/1: Batch 281/782: Batch Loss: 0.7869155406951904\n",
            "Epoch 0/1: Batch 282/782: Batch Loss: 0.487975537776947\n",
            "Epoch 0/1: Batch 283/782: Batch Loss: 0.48831889033317566\n",
            "Epoch 0/1: Batch 284/782: Batch Loss: 0.9530308246612549\n",
            "Epoch 0/1: Batch 285/782: Batch Loss: 0.5220361351966858\n",
            "Epoch 0/1: Batch 286/782: Batch Loss: 0.7212152481079102\n",
            "Epoch 0/1: Batch 287/782: Batch Loss: 0.7109327912330627\n",
            "Epoch 0/1: Batch 288/782: Batch Loss: 0.6224671006202698\n",
            "Epoch 0/1: Batch 289/782: Batch Loss: 0.5238932371139526\n",
            "Epoch 0/1: Batch 290/782: Batch Loss: 0.593442976474762\n",
            "Epoch 0/1: Batch 291/782: Batch Loss: 0.7865825891494751\n",
            "Epoch 0/1: Batch 292/782: Batch Loss: 0.5029776096343994\n",
            "Epoch 0/1: Batch 293/782: Batch Loss: 0.746663510799408\n",
            "Epoch 0/1: Batch 294/782: Batch Loss: 0.5344613790512085\n",
            "Epoch 0/1: Batch 295/782: Batch Loss: 0.5958475470542908\n",
            "Epoch 0/1: Batch 296/782: Batch Loss: 0.7639306783676147\n",
            "Epoch 0/1: Batch 297/782: Batch Loss: 0.6950452923774719\n",
            "Epoch 0/1: Batch 298/782: Batch Loss: 0.5605062246322632\n",
            "Epoch 0/1: Batch 299/782: Batch Loss: 0.47464150190353394\n",
            "Epoch 0/1: Batch 300/782: Batch Loss: 0.9225183129310608\n",
            "Epoch 0/1: Batch 301/782: Batch Loss: 0.6896796226501465\n",
            "Epoch 0/1: Batch 302/782: Batch Loss: 0.7527168989181519\n",
            "Epoch 0/1: Batch 303/782: Batch Loss: 0.8134168386459351\n",
            "Epoch 0/1: Batch 304/782: Batch Loss: 0.5748507976531982\n",
            "Epoch 0/1: Batch 305/782: Batch Loss: 0.7184040546417236\n",
            "Epoch 0/1: Batch 306/782: Batch Loss: 0.7109395861625671\n",
            "Epoch 0/1: Batch 307/782: Batch Loss: 0.6088466048240662\n",
            "Epoch 0/1: Batch 308/782: Batch Loss: 0.8302801847457886\n",
            "Epoch 0/1: Batch 309/782: Batch Loss: 0.5463671684265137\n",
            "Epoch 0/1: Batch 310/782: Batch Loss: 0.7529209852218628\n",
            "Epoch 0/1: Batch 311/782: Batch Loss: 0.6452559232711792\n",
            "Epoch 0/1: Batch 312/782: Batch Loss: 0.6185407638549805\n",
            "Epoch 0/1: Batch 313/782: Batch Loss: 0.5441263318061829\n",
            "Epoch 0/1: Batch 314/782: Batch Loss: 0.5362738370895386\n",
            "Epoch 0/1: Batch 315/782: Batch Loss: 0.6860435009002686\n",
            "Epoch 0/1: Batch 316/782: Batch Loss: 0.5373882055282593\n",
            "Epoch 0/1: Batch 317/782: Batch Loss: 0.6680846810340881\n",
            "Epoch 0/1: Batch 318/782: Batch Loss: 0.6312886476516724\n",
            "Epoch 0/1: Batch 319/782: Batch Loss: 0.840555727481842\n",
            "Epoch 0/1: Batch 320/782: Batch Loss: 0.6874724626541138\n",
            "Epoch 0/1: Batch 321/782: Batch Loss: 0.6361508369445801\n",
            "Epoch 0/1: Batch 322/782: Batch Loss: 0.7324001789093018\n",
            "Epoch 0/1: Batch 323/782: Batch Loss: 0.6158014535903931\n",
            "Epoch 0/1: Batch 324/782: Batch Loss: 0.8347790241241455\n",
            "Epoch 0/1: Batch 325/782: Batch Loss: 0.8820691704750061\n",
            "Epoch 0/1: Batch 326/782: Batch Loss: 0.7529222965240479\n",
            "Epoch 0/1: Batch 327/782: Batch Loss: 0.6367648839950562\n",
            "Epoch 0/1: Batch 328/782: Batch Loss: 0.754540205001831\n",
            "Epoch 0/1: Batch 329/782: Batch Loss: 0.709993302822113\n",
            "Epoch 0/1: Batch 330/782: Batch Loss: 0.6122049689292908\n",
            "Epoch 0/1: Batch 331/782: Batch Loss: 0.8101992011070251\n",
            "Epoch 0/1: Batch 332/782: Batch Loss: 0.6629114747047424\n",
            "Epoch 0/1: Batch 333/782: Batch Loss: 0.5973212122917175\n",
            "Epoch 0/1: Batch 334/782: Batch Loss: 0.5501942038536072\n",
            "Epoch 0/1: Batch 335/782: Batch Loss: 0.5754350423812866\n",
            "Epoch 0/1: Batch 336/782: Batch Loss: 0.6065785884857178\n",
            "Epoch 0/1: Batch 337/782: Batch Loss: 0.7680931091308594\n",
            "Epoch 0/1: Batch 338/782: Batch Loss: 0.7936444878578186\n",
            "Epoch 0/1: Batch 339/782: Batch Loss: 0.5007373094558716\n",
            "Epoch 0/1: Batch 340/782: Batch Loss: 0.7747172713279724\n",
            "Epoch 0/1: Batch 341/782: Batch Loss: 0.5279667973518372\n",
            "Epoch 0/1: Batch 342/782: Batch Loss: 0.7939507365226746\n",
            "Epoch 0/1: Batch 343/782: Batch Loss: 0.6167731881141663\n",
            "Epoch 0/1: Batch 344/782: Batch Loss: 0.5906642079353333\n",
            "Epoch 0/1: Batch 345/782: Batch Loss: 0.6112433075904846\n",
            "Epoch 0/1: Batch 346/782: Batch Loss: 0.6881482601165771\n",
            "Epoch 0/1: Batch 347/782: Batch Loss: 0.6084170937538147\n",
            "Epoch 0/1: Batch 348/782: Batch Loss: 0.5145269632339478\n",
            "Epoch 0/1: Batch 349/782: Batch Loss: 0.5748967528343201\n",
            "Epoch 0/1: Batch 350/782: Batch Loss: 0.8868424892425537\n",
            "Epoch 0/1: Batch 351/782: Batch Loss: 0.7575154900550842\n",
            "Epoch 0/1: Batch 352/782: Batch Loss: 0.6549952626228333\n",
            "Epoch 0/1: Batch 353/782: Batch Loss: 0.8156368136405945\n",
            "Epoch 0/1: Batch 354/782: Batch Loss: 0.49459537863731384\n",
            "Epoch 0/1: Batch 355/782: Batch Loss: 0.6788746118545532\n",
            "Epoch 0/1: Batch 356/782: Batch Loss: 0.9705694913864136\n",
            "Epoch 0/1: Batch 357/782: Batch Loss: 0.8053510189056396\n",
            "Epoch 0/1: Batch 358/782: Batch Loss: 0.5605919361114502\n",
            "Epoch 0/1: Batch 359/782: Batch Loss: 0.5367372632026672\n",
            "Epoch 0/1: Batch 360/782: Batch Loss: 0.5813846588134766\n",
            "Epoch 0/1: Batch 361/782: Batch Loss: 0.6532575488090515\n",
            "Epoch 0/1: Batch 362/782: Batch Loss: 0.5799640417098999\n",
            "Epoch 0/1: Batch 363/782: Batch Loss: 0.6639987230300903\n",
            "Epoch 0/1: Batch 364/782: Batch Loss: 0.6258733868598938\n",
            "Epoch 0/1: Batch 365/782: Batch Loss: 0.542995810508728\n",
            "Epoch 0/1: Batch 366/782: Batch Loss: 0.6605151295661926\n",
            "Epoch 0/1: Batch 367/782: Batch Loss: 0.633537769317627\n",
            "Epoch 0/1: Batch 368/782: Batch Loss: 0.5210280418395996\n",
            "Epoch 0/1: Batch 369/782: Batch Loss: 0.6512561440467834\n",
            "Epoch 0/1: Batch 370/782: Batch Loss: 0.6138856410980225\n",
            "Epoch 0/1: Batch 371/782: Batch Loss: 0.5671586394309998\n",
            "Epoch 0/1: Batch 372/782: Batch Loss: 0.5724671483039856\n",
            "Epoch 0/1: Batch 373/782: Batch Loss: 0.5030414462089539\n",
            "Epoch 0/1: Batch 374/782: Batch Loss: 0.8539021611213684\n",
            "Epoch 0/1: Batch 375/782: Batch Loss: 0.7003490328788757\n",
            "Epoch 0/1: Batch 376/782: Batch Loss: 0.6973418593406677\n",
            "Epoch 0/1: Batch 377/782: Batch Loss: 0.7061700224876404\n",
            "Epoch 0/1: Batch 378/782: Batch Loss: 0.8244109153747559\n",
            "Epoch 0/1: Batch 379/782: Batch Loss: 0.6872497200965881\n",
            "Epoch 0/1: Batch 380/782: Batch Loss: 0.5874351859092712\n",
            "Epoch 0/1: Batch 381/782: Batch Loss: 0.8816865682601929\n",
            "Epoch 0/1: Batch 382/782: Batch Loss: 0.733203649520874\n",
            "Epoch 0/1: Batch 383/782: Batch Loss: 0.4173513650894165\n",
            "Epoch 0/1: Batch 384/782: Batch Loss: 0.6461142897605896\n",
            "Epoch 0/1: Batch 385/782: Batch Loss: 0.7001185417175293\n",
            "Epoch 0/1: Batch 386/782: Batch Loss: 0.7239330410957336\n",
            "Epoch 0/1: Batch 387/782: Batch Loss: 0.7926546335220337\n",
            "Epoch 0/1: Batch 388/782: Batch Loss: 0.3733034133911133\n",
            "Epoch 0/1: Batch 389/782: Batch Loss: 0.6282377243041992\n",
            "Epoch 0/1: Batch 390/782: Batch Loss: 0.6557217836380005\n",
            "Epoch 0/1: Batch 391/782: Batch Loss: 0.7047892212867737\n",
            "Epoch 0/1: Batch 392/782: Batch Loss: 0.40127959847450256\n",
            "Epoch 0/1: Batch 393/782: Batch Loss: 0.5618569254875183\n",
            "Epoch 0/1: Batch 394/782: Batch Loss: 0.6412690281867981\n",
            "Epoch 0/1: Batch 395/782: Batch Loss: 0.6909986138343811\n",
            "Epoch 0/1: Batch 396/782: Batch Loss: 0.7697679996490479\n",
            "Epoch 0/1: Batch 397/782: Batch Loss: 0.7704418897628784\n",
            "Epoch 0/1: Batch 398/782: Batch Loss: 0.6998857855796814\n",
            "Epoch 0/1: Batch 399/782: Batch Loss: 0.4949008822441101\n",
            "Epoch 0/1: Batch 400/782: Batch Loss: 0.5776393413543701\n",
            "Epoch 0/1: Batch 401/782: Batch Loss: 0.6158434748649597\n",
            "Epoch 0/1: Batch 402/782: Batch Loss: 0.8108197450637817\n",
            "Epoch 0/1: Batch 403/782: Batch Loss: 0.7375056147575378\n",
            "Epoch 0/1: Batch 404/782: Batch Loss: 0.9489783048629761\n",
            "Epoch 0/1: Batch 405/782: Batch Loss: 0.6729411482810974\n",
            "Epoch 0/1: Batch 406/782: Batch Loss: 0.6666305661201477\n",
            "Epoch 0/1: Batch 407/782: Batch Loss: 0.7720892429351807\n",
            "Epoch 0/1: Batch 408/782: Batch Loss: 0.5476449131965637\n",
            "Epoch 0/1: Batch 409/782: Batch Loss: 0.5229784250259399\n",
            "Epoch 0/1: Batch 410/782: Batch Loss: 0.4948776364326477\n",
            "Epoch 0/1: Batch 411/782: Batch Loss: 0.6240040063858032\n",
            "Epoch 0/1: Batch 412/782: Batch Loss: 0.8598917126655579\n",
            "Epoch 0/1: Batch 413/782: Batch Loss: 0.4744415283203125\n",
            "Epoch 0/1: Batch 414/782: Batch Loss: 0.743085503578186\n",
            "Epoch 0/1: Batch 415/782: Batch Loss: 0.5914345383644104\n",
            "Epoch 0/1: Batch 416/782: Batch Loss: 0.7289695143699646\n",
            "Epoch 0/1: Batch 417/782: Batch Loss: 0.5011206865310669\n",
            "Epoch 0/1: Batch 418/782: Batch Loss: 0.7673833966255188\n",
            "Epoch 0/1: Batch 419/782: Batch Loss: 0.7099246382713318\n",
            "Epoch 0/1: Batch 420/782: Batch Loss: 0.6163378357887268\n",
            "Epoch 0/1: Batch 421/782: Batch Loss: 0.7410180568695068\n",
            "Epoch 0/1: Batch 422/782: Batch Loss: 0.47268086671829224\n",
            "Epoch 0/1: Batch 423/782: Batch Loss: 0.9378653764724731\n",
            "Epoch 0/1: Batch 424/782: Batch Loss: 0.5253233313560486\n",
            "Epoch 0/1: Batch 425/782: Batch Loss: 0.6155259013175964\n",
            "Epoch 0/1: Batch 426/782: Batch Loss: 0.7670108675956726\n",
            "Epoch 0/1: Batch 427/782: Batch Loss: 0.7382978200912476\n",
            "Epoch 0/1: Batch 428/782: Batch Loss: 0.7052342295646667\n",
            "Epoch 0/1: Batch 429/782: Batch Loss: 0.6109817028045654\n",
            "Epoch 0/1: Batch 430/782: Batch Loss: 0.869683027267456\n",
            "Epoch 0/1: Batch 431/782: Batch Loss: 0.4598030745983124\n",
            "Epoch 0/1: Batch 432/782: Batch Loss: 0.751305878162384\n",
            "Epoch 0/1: Batch 433/782: Batch Loss: 0.7831830978393555\n",
            "Epoch 0/1: Batch 434/782: Batch Loss: 0.5342786908149719\n",
            "Epoch 0/1: Batch 435/782: Batch Loss: 0.6304469704627991\n",
            "Epoch 0/1: Batch 436/782: Batch Loss: 0.440450519323349\n",
            "Epoch 0/1: Batch 437/782: Batch Loss: 0.7126132845878601\n",
            "Epoch 0/1: Batch 438/782: Batch Loss: 0.7151191234588623\n",
            "Epoch 0/1: Batch 439/782: Batch Loss: 0.8296160697937012\n",
            "Epoch 0/1: Batch 440/782: Batch Loss: 0.7887862324714661\n",
            "Epoch 0/1: Batch 441/782: Batch Loss: 0.4827076494693756\n",
            "Epoch 0/1: Batch 442/782: Batch Loss: 0.6437394022941589\n",
            "Epoch 0/1: Batch 443/782: Batch Loss: 0.4657846987247467\n",
            "Epoch 0/1: Batch 444/782: Batch Loss: 0.7155025601387024\n",
            "Epoch 0/1: Batch 445/782: Batch Loss: 0.5348362922668457\n",
            "Epoch 0/1: Batch 446/782: Batch Loss: 0.5700366497039795\n",
            "Epoch 0/1: Batch 447/782: Batch Loss: 0.5994297862052917\n",
            "Epoch 0/1: Batch 448/782: Batch Loss: 0.6087955236434937\n",
            "Epoch 0/1: Batch 449/782: Batch Loss: 0.7070730328559875\n",
            "Epoch 0/1: Batch 450/782: Batch Loss: 0.7093089818954468\n",
            "Epoch 0/1: Batch 451/782: Batch Loss: 0.7329587936401367\n",
            "Epoch 0/1: Batch 452/782: Batch Loss: 0.7051146626472473\n",
            "Epoch 0/1: Batch 453/782: Batch Loss: 0.6919751167297363\n",
            "Epoch 0/1: Batch 454/782: Batch Loss: 0.5559488534927368\n",
            "Epoch 0/1: Batch 455/782: Batch Loss: 0.58111172914505\n",
            "Epoch 0/1: Batch 456/782: Batch Loss: 0.5475826263427734\n",
            "Epoch 0/1: Batch 457/782: Batch Loss: 0.6479261517524719\n",
            "Epoch 0/1: Batch 458/782: Batch Loss: 0.7647335529327393\n",
            "Epoch 0/1: Batch 459/782: Batch Loss: 0.6548356413841248\n",
            "Epoch 0/1: Batch 460/782: Batch Loss: 0.7027563452720642\n",
            "Epoch 0/1: Batch 461/782: Batch Loss: 0.4576670229434967\n",
            "Epoch 0/1: Batch 462/782: Batch Loss: 0.6429190039634705\n",
            "Epoch 0/1: Batch 463/782: Batch Loss: 0.5917055010795593\n",
            "Epoch 0/1: Batch 464/782: Batch Loss: 0.6329001784324646\n",
            "Epoch 0/1: Batch 465/782: Batch Loss: 0.779558002948761\n",
            "Epoch 0/1: Batch 466/782: Batch Loss: 0.5930565595626831\n",
            "Epoch 0/1: Batch 467/782: Batch Loss: 0.650122344493866\n",
            "Epoch 0/1: Batch 468/782: Batch Loss: 0.6843265295028687\n",
            "Epoch 0/1: Batch 469/782: Batch Loss: 0.3647294044494629\n",
            "Epoch 0/1: Batch 470/782: Batch Loss: 0.7805221080780029\n",
            "Epoch 0/1: Batch 471/782: Batch Loss: 0.5757554769515991\n",
            "Epoch 0/1: Batch 472/782: Batch Loss: 0.46574223041534424\n",
            "Epoch 0/1: Batch 473/782: Batch Loss: 0.6010947823524475\n",
            "Epoch 0/1: Batch 474/782: Batch Loss: 0.6834768056869507\n",
            "Epoch 0/1: Batch 475/782: Batch Loss: 0.3882329761981964\n",
            "Epoch 0/1: Batch 476/782: Batch Loss: 0.6177826523780823\n",
            "Epoch 0/1: Batch 477/782: Batch Loss: 0.3757345378398895\n",
            "Epoch 0/1: Batch 478/782: Batch Loss: 0.5816209316253662\n",
            "Epoch 0/1: Batch 479/782: Batch Loss: 0.6933974027633667\n",
            "Epoch 0/1: Batch 480/782: Batch Loss: 0.5442622900009155\n",
            "Epoch 0/1: Batch 481/782: Batch Loss: 0.5287216901779175\n",
            "Epoch 0/1: Batch 482/782: Batch Loss: 0.7533044815063477\n",
            "Epoch 0/1: Batch 483/782: Batch Loss: 0.5917611122131348\n",
            "Epoch 0/1: Batch 484/782: Batch Loss: 0.4861302375793457\n",
            "Epoch 0/1: Batch 485/782: Batch Loss: 0.5872746706008911\n",
            "Epoch 0/1: Batch 486/782: Batch Loss: 0.7877916097640991\n",
            "Epoch 0/1: Batch 487/782: Batch Loss: 0.6929929256439209\n",
            "Epoch 0/1: Batch 488/782: Batch Loss: 0.5892390012741089\n",
            "Epoch 0/1: Batch 489/782: Batch Loss: 0.5436307787895203\n",
            "Epoch 0/1: Batch 490/782: Batch Loss: 0.7720363140106201\n",
            "Epoch 0/1: Batch 491/782: Batch Loss: 0.7919133305549622\n",
            "Epoch 0/1: Batch 492/782: Batch Loss: 0.5854518413543701\n",
            "Epoch 0/1: Batch 493/782: Batch Loss: 0.5883299708366394\n",
            "Epoch 0/1: Batch 494/782: Batch Loss: 0.5974822640419006\n",
            "Epoch 0/1: Batch 495/782: Batch Loss: 0.6191758513450623\n",
            "Epoch 0/1: Batch 496/782: Batch Loss: 0.6124166250228882\n",
            "Epoch 0/1: Batch 497/782: Batch Loss: 0.7117548584938049\n",
            "Epoch 0/1: Batch 498/782: Batch Loss: 0.8485612869262695\n",
            "Epoch 0/1: Batch 499/782: Batch Loss: 0.9643454551696777\n",
            "Epoch 0/1: Batch 500/782: Batch Loss: 0.5387453436851501\n",
            "Epoch 0/1: Batch 501/782: Batch Loss: 0.746900737285614\n",
            "Epoch 0/1: Batch 502/782: Batch Loss: 0.5176280736923218\n",
            "Epoch 0/1: Batch 503/782: Batch Loss: 0.6618998050689697\n",
            "Epoch 0/1: Batch 504/782: Batch Loss: 0.7372485995292664\n",
            "Epoch 0/1: Batch 505/782: Batch Loss: 0.6252228021621704\n",
            "Epoch 0/1: Batch 506/782: Batch Loss: 0.620266854763031\n",
            "Epoch 0/1: Batch 507/782: Batch Loss: 0.4459344446659088\n",
            "Epoch 0/1: Batch 508/782: Batch Loss: 0.7723231911659241\n",
            "Epoch 0/1: Batch 509/782: Batch Loss: 0.7464603781700134\n",
            "Epoch 0/1: Batch 510/782: Batch Loss: 0.6633795499801636\n",
            "Epoch 0/1: Batch 511/782: Batch Loss: 0.5361840724945068\n",
            "Epoch 0/1: Batch 512/782: Batch Loss: 0.47376492619514465\n",
            "Epoch 0/1: Batch 513/782: Batch Loss: 0.7750074863433838\n",
            "Epoch 0/1: Batch 514/782: Batch Loss: 0.7572613954544067\n",
            "Epoch 0/1: Batch 515/782: Batch Loss: 0.5337834358215332\n",
            "Epoch 0/1: Batch 516/782: Batch Loss: 1.0442322492599487\n",
            "Epoch 0/1: Batch 517/782: Batch Loss: 0.5568335652351379\n",
            "Epoch 0/1: Batch 518/782: Batch Loss: 0.8924928307533264\n",
            "Epoch 0/1: Batch 519/782: Batch Loss: 0.5990260243415833\n",
            "Epoch 0/1: Batch 520/782: Batch Loss: 0.5672795176506042\n",
            "Epoch 0/1: Batch 521/782: Batch Loss: 0.4252033531665802\n",
            "Epoch 0/1: Batch 522/782: Batch Loss: 0.729142427444458\n",
            "Epoch 0/1: Batch 523/782: Batch Loss: 0.5182759165763855\n",
            "Epoch 0/1: Batch 524/782: Batch Loss: 0.6124876141548157\n",
            "Epoch 0/1: Batch 525/782: Batch Loss: 0.6004796624183655\n",
            "Epoch 0/1: Batch 526/782: Batch Loss: 0.8397259712219238\n",
            "Epoch 0/1: Batch 527/782: Batch Loss: 0.6156629920005798\n",
            "Epoch 0/1: Batch 528/782: Batch Loss: 0.5648996233940125\n",
            "Epoch 0/1: Batch 529/782: Batch Loss: 0.6593278050422668\n",
            "Epoch 0/1: Batch 530/782: Batch Loss: 0.6898466348648071\n",
            "Epoch 0/1: Batch 531/782: Batch Loss: 0.5348052382469177\n",
            "Epoch 0/1: Batch 532/782: Batch Loss: 0.6530213356018066\n",
            "Epoch 0/1: Batch 533/782: Batch Loss: 0.5373547673225403\n",
            "Epoch 0/1: Batch 534/782: Batch Loss: 0.6302940249443054\n",
            "Epoch 0/1: Batch 535/782: Batch Loss: 0.4967191517353058\n",
            "Epoch 0/1: Batch 536/782: Batch Loss: 0.5972242951393127\n",
            "Epoch 0/1: Batch 537/782: Batch Loss: 0.6096084117889404\n",
            "Epoch 0/1: Batch 538/782: Batch Loss: 0.7280890941619873\n",
            "Epoch 0/1: Batch 539/782: Batch Loss: 0.5207446217536926\n",
            "Epoch 0/1: Batch 540/782: Batch Loss: 0.4465939998626709\n",
            "Epoch 0/1: Batch 541/782: Batch Loss: 0.47257596254348755\n",
            "Epoch 0/1: Batch 542/782: Batch Loss: 0.7019149661064148\n",
            "Epoch 0/1: Batch 543/782: Batch Loss: 0.49370232224464417\n",
            "Epoch 0/1: Batch 544/782: Batch Loss: 0.49798715114593506\n",
            "Epoch 0/1: Batch 545/782: Batch Loss: 0.6698645949363708\n",
            "Epoch 0/1: Batch 546/782: Batch Loss: 0.9287504553794861\n",
            "Epoch 0/1: Batch 547/782: Batch Loss: 0.7025038003921509\n",
            "Epoch 0/1: Batch 548/782: Batch Loss: 0.6966774463653564\n",
            "Epoch 0/1: Batch 549/782: Batch Loss: 0.5762599110603333\n",
            "Epoch 0/1: Batch 550/782: Batch Loss: 0.48795008659362793\n",
            "Epoch 0/1: Batch 551/782: Batch Loss: 0.6789315938949585\n",
            "Epoch 0/1: Batch 552/782: Batch Loss: 0.48124024271965027\n",
            "Epoch 0/1: Batch 553/782: Batch Loss: 0.6040509343147278\n",
            "Epoch 0/1: Batch 554/782: Batch Loss: 0.5563662052154541\n",
            "Epoch 0/1: Batch 555/782: Batch Loss: 0.6308320760726929\n",
            "Epoch 0/1: Batch 556/782: Batch Loss: 0.5760118365287781\n",
            "Epoch 0/1: Batch 557/782: Batch Loss: 0.6199420094490051\n",
            "Epoch 0/1: Batch 558/782: Batch Loss: 0.5443562269210815\n",
            "Epoch 0/1: Batch 559/782: Batch Loss: 0.5795204043388367\n",
            "Epoch 0/1: Batch 560/782: Batch Loss: 0.7055076956748962\n",
            "Epoch 0/1: Batch 561/782: Batch Loss: 0.8440532684326172\n",
            "Epoch 0/1: Batch 562/782: Batch Loss: 0.5958850383758545\n",
            "Epoch 0/1: Batch 563/782: Batch Loss: 0.4728621244430542\n",
            "Epoch 0/1: Batch 564/782: Batch Loss: 0.9029829502105713\n",
            "Epoch 0/1: Batch 565/782: Batch Loss: 0.7190884351730347\n",
            "Epoch 0/1: Batch 566/782: Batch Loss: 0.39166000485420227\n",
            "Epoch 0/1: Batch 567/782: Batch Loss: 0.7433364391326904\n",
            "Epoch 0/1: Batch 568/782: Batch Loss: 0.5954261422157288\n",
            "Epoch 0/1: Batch 569/782: Batch Loss: 0.7246948480606079\n",
            "Epoch 0/1: Batch 570/782: Batch Loss: 0.6288226246833801\n",
            "Epoch 0/1: Batch 571/782: Batch Loss: 0.5608644485473633\n",
            "Epoch 0/1: Batch 572/782: Batch Loss: 0.5443586707115173\n",
            "Epoch 0/1: Batch 573/782: Batch Loss: 0.6618898510932922\n",
            "Epoch 0/1: Batch 574/782: Batch Loss: 0.7152365446090698\n",
            "Epoch 0/1: Batch 575/782: Batch Loss: 0.7373483777046204\n",
            "Epoch 0/1: Batch 576/782: Batch Loss: 0.5267900228500366\n",
            "Epoch 0/1: Batch 577/782: Batch Loss: 0.6592612862586975\n",
            "Epoch 0/1: Batch 578/782: Batch Loss: 0.586834728717804\n",
            "Epoch 0/1: Batch 579/782: Batch Loss: 0.6705355644226074\n",
            "Epoch 0/1: Batch 580/782: Batch Loss: 0.6446383595466614\n",
            "Epoch 0/1: Batch 581/782: Batch Loss: 0.6621357798576355\n",
            "Epoch 0/1: Batch 582/782: Batch Loss: 0.5850577354431152\n",
            "Epoch 0/1: Batch 583/782: Batch Loss: 0.6101725697517395\n",
            "Epoch 0/1: Batch 584/782: Batch Loss: 0.5679495334625244\n",
            "Epoch 0/1: Batch 585/782: Batch Loss: 0.3846876621246338\n",
            "Epoch 0/1: Batch 586/782: Batch Loss: 0.4149802327156067\n",
            "Epoch 0/1: Batch 587/782: Batch Loss: 0.6892223358154297\n",
            "Epoch 0/1: Batch 588/782: Batch Loss: 0.797258198261261\n",
            "Epoch 0/1: Batch 589/782: Batch Loss: 0.7651849389076233\n",
            "Epoch 0/1: Batch 590/782: Batch Loss: 1.102184772491455\n",
            "Epoch 0/1: Batch 591/782: Batch Loss: 0.55183345079422\n",
            "Epoch 0/1: Batch 592/782: Batch Loss: 0.5556859374046326\n",
            "Epoch 0/1: Batch 593/782: Batch Loss: 0.4404263496398926\n",
            "Epoch 0/1: Batch 594/782: Batch Loss: 0.7393465042114258\n",
            "Epoch 0/1: Batch 595/782: Batch Loss: 0.664036750793457\n",
            "Epoch 0/1: Batch 596/782: Batch Loss: 0.6762839555740356\n",
            "Epoch 0/1: Batch 597/782: Batch Loss: 0.8853090405464172\n",
            "Epoch 0/1: Batch 598/782: Batch Loss: 0.4723495841026306\n",
            "Epoch 0/1: Batch 599/782: Batch Loss: 0.7649709582328796\n",
            "Epoch 0/1: Batch 600/782: Batch Loss: 0.8177928924560547\n",
            "Epoch 0/1: Batch 601/782: Batch Loss: 0.6984078288078308\n",
            "Epoch 0/1: Batch 602/782: Batch Loss: 0.6169217824935913\n",
            "Epoch 0/1: Batch 603/782: Batch Loss: 0.6725200414657593\n",
            "Epoch 0/1: Batch 604/782: Batch Loss: 0.5676662921905518\n",
            "Epoch 0/1: Batch 605/782: Batch Loss: 0.6871306300163269\n",
            "Epoch 0/1: Batch 606/782: Batch Loss: 0.6490843892097473\n",
            "Epoch 0/1: Batch 607/782: Batch Loss: 0.48732900619506836\n",
            "Epoch 0/1: Batch 608/782: Batch Loss: 0.5258932113647461\n",
            "Epoch 0/1: Batch 609/782: Batch Loss: 0.6958468556404114\n",
            "Epoch 0/1: Batch 610/782: Batch Loss: 0.6732257604598999\n",
            "Epoch 0/1: Batch 611/782: Batch Loss: 0.5497803092002869\n",
            "Epoch 0/1: Batch 612/782: Batch Loss: 0.576416015625\n",
            "Epoch 0/1: Batch 613/782: Batch Loss: 0.655896782875061\n",
            "Epoch 0/1: Batch 614/782: Batch Loss: 0.691392719745636\n",
            "Epoch 0/1: Batch 615/782: Batch Loss: 0.7599470615386963\n",
            "Epoch 0/1: Batch 616/782: Batch Loss: 0.5749964714050293\n",
            "Epoch 0/1: Batch 617/782: Batch Loss: 0.9015198945999146\n",
            "Epoch 0/1: Batch 618/782: Batch Loss: 0.6878332495689392\n",
            "Epoch 0/1: Batch 619/782: Batch Loss: 0.5689902901649475\n",
            "Epoch 0/1: Batch 620/782: Batch Loss: 0.6821982860565186\n",
            "Epoch 0/1: Batch 621/782: Batch Loss: 0.7710332870483398\n",
            "Epoch 0/1: Batch 622/782: Batch Loss: 0.579739511013031\n",
            "Epoch 0/1: Batch 623/782: Batch Loss: 0.5855091214179993\n",
            "Epoch 0/1: Batch 624/782: Batch Loss: 0.4940493702888489\n",
            "Epoch 0/1: Batch 625/782: Batch Loss: 0.7301081418991089\n",
            "Epoch 0/1: Batch 626/782: Batch Loss: 0.6373875737190247\n",
            "Epoch 0/1: Batch 627/782: Batch Loss: 0.9216079115867615\n",
            "Epoch 0/1: Batch 628/782: Batch Loss: 0.4260540306568146\n",
            "Epoch 0/1: Batch 629/782: Batch Loss: 0.7075433731079102\n",
            "Epoch 0/1: Batch 630/782: Batch Loss: 0.7119752764701843\n",
            "Epoch 0/1: Batch 631/782: Batch Loss: 0.6099647879600525\n",
            "Epoch 0/1: Batch 632/782: Batch Loss: 0.4605580270290375\n",
            "Epoch 0/1: Batch 633/782: Batch Loss: 0.42940813302993774\n",
            "Epoch 0/1: Batch 634/782: Batch Loss: 0.5989651083946228\n",
            "Epoch 0/1: Batch 635/782: Batch Loss: 0.6164637207984924\n",
            "Epoch 0/1: Batch 636/782: Batch Loss: 0.7475011944770813\n",
            "Epoch 0/1: Batch 637/782: Batch Loss: 0.6334485411643982\n",
            "Epoch 0/1: Batch 638/782: Batch Loss: 0.7418522238731384\n",
            "Epoch 0/1: Batch 639/782: Batch Loss: 0.5168130397796631\n",
            "Epoch 0/1: Batch 640/782: Batch Loss: 0.642912745475769\n",
            "Epoch 0/1: Batch 641/782: Batch Loss: 0.5093210339546204\n",
            "Epoch 0/1: Batch 642/782: Batch Loss: 0.5414578318595886\n",
            "Epoch 0/1: Batch 643/782: Batch Loss: 0.5810307264328003\n",
            "Epoch 0/1: Batch 644/782: Batch Loss: 0.665518045425415\n",
            "Epoch 0/1: Batch 645/782: Batch Loss: 0.3782755136489868\n",
            "Epoch 0/1: Batch 646/782: Batch Loss: 0.5280252695083618\n",
            "Epoch 0/1: Batch 647/782: Batch Loss: 0.5062761306762695\n",
            "Epoch 0/1: Batch 648/782: Batch Loss: 0.6619519591331482\n",
            "Epoch 0/1: Batch 649/782: Batch Loss: 0.5536108613014221\n",
            "Epoch 0/1: Batch 650/782: Batch Loss: 0.8878984451293945\n",
            "Epoch 0/1: Batch 651/782: Batch Loss: 0.5751869082450867\n",
            "Epoch 0/1: Batch 652/782: Batch Loss: 0.47736912965774536\n",
            "Epoch 0/1: Batch 653/782: Batch Loss: 0.7131579518318176\n",
            "Epoch 0/1: Batch 654/782: Batch Loss: 0.6217950582504272\n",
            "Epoch 0/1: Batch 655/782: Batch Loss: 0.5612046718597412\n",
            "Epoch 0/1: Batch 656/782: Batch Loss: 0.9026564955711365\n",
            "Epoch 0/1: Batch 657/782: Batch Loss: 0.7062243819236755\n",
            "Epoch 0/1: Batch 658/782: Batch Loss: 0.626339316368103\n",
            "Epoch 0/1: Batch 659/782: Batch Loss: 0.5831190943717957\n",
            "Epoch 0/1: Batch 660/782: Batch Loss: 0.47355031967163086\n",
            "Epoch 0/1: Batch 661/782: Batch Loss: 0.5460607409477234\n",
            "Epoch 0/1: Batch 662/782: Batch Loss: 0.6262193322181702\n",
            "Epoch 0/1: Batch 663/782: Batch Loss: 0.6857161521911621\n",
            "Epoch 0/1: Batch 664/782: Batch Loss: 0.6096091866493225\n",
            "Epoch 0/1: Batch 665/782: Batch Loss: 0.6067551374435425\n",
            "Epoch 0/1: Batch 666/782: Batch Loss: 0.6147394180297852\n",
            "Epoch 0/1: Batch 667/782: Batch Loss: 0.4738677144050598\n",
            "Epoch 0/1: Batch 668/782: Batch Loss: 0.5672518014907837\n",
            "Epoch 0/1: Batch 669/782: Batch Loss: 0.5679817795753479\n",
            "Epoch 0/1: Batch 670/782: Batch Loss: 0.4876762330532074\n",
            "Epoch 0/1: Batch 671/782: Batch Loss: 0.5488671064376831\n",
            "Epoch 0/1: Batch 672/782: Batch Loss: 0.7593679428100586\n",
            "Epoch 0/1: Batch 673/782: Batch Loss: 0.6648889780044556\n",
            "Epoch 0/1: Batch 674/782: Batch Loss: 0.40002256631851196\n",
            "Epoch 0/1: Batch 675/782: Batch Loss: 0.5299767851829529\n",
            "Epoch 0/1: Batch 676/782: Batch Loss: 0.510398805141449\n",
            "Epoch 0/1: Batch 677/782: Batch Loss: 0.4849607050418854\n",
            "Epoch 0/1: Batch 678/782: Batch Loss: 0.5908111929893494\n",
            "Epoch 0/1: Batch 679/782: Batch Loss: 0.8291434049606323\n",
            "Epoch 0/1: Batch 680/782: Batch Loss: 0.5514138340950012\n",
            "Epoch 0/1: Batch 681/782: Batch Loss: 0.7108196020126343\n",
            "Epoch 0/1: Batch 682/782: Batch Loss: 0.5079604387283325\n",
            "Epoch 0/1: Batch 683/782: Batch Loss: 0.6806617975234985\n",
            "Epoch 0/1: Batch 684/782: Batch Loss: 0.7723063826560974\n",
            "Epoch 0/1: Batch 685/782: Batch Loss: 0.6444193124771118\n",
            "Epoch 0/1: Batch 686/782: Batch Loss: 0.7365932464599609\n",
            "Epoch 0/1: Batch 687/782: Batch Loss: 0.9017588496208191\n",
            "Epoch 0/1: Batch 688/782: Batch Loss: 0.7131590843200684\n",
            "Epoch 0/1: Batch 689/782: Batch Loss: 0.5851112008094788\n",
            "Epoch 0/1: Batch 690/782: Batch Loss: 0.5988719463348389\n",
            "Epoch 0/1: Batch 691/782: Batch Loss: 0.7866571545600891\n",
            "Epoch 0/1: Batch 692/782: Batch Loss: 0.5652874112129211\n",
            "Epoch 0/1: Batch 693/782: Batch Loss: 0.6207728981971741\n",
            "Epoch 0/1: Batch 694/782: Batch Loss: 0.741905689239502\n",
            "Epoch 0/1: Batch 695/782: Batch Loss: 0.5253859758377075\n",
            "Epoch 0/1: Batch 696/782: Batch Loss: 0.6000714302062988\n",
            "Epoch 0/1: Batch 697/782: Batch Loss: 0.6805852055549622\n",
            "Epoch 0/1: Batch 698/782: Batch Loss: 0.5916019678115845\n",
            "Epoch 0/1: Batch 699/782: Batch Loss: 0.6855478286743164\n",
            "Epoch 0/1: Batch 700/782: Batch Loss: 0.5413110256195068\n",
            "Epoch 0/1: Batch 701/782: Batch Loss: 0.5955674052238464\n",
            "Epoch 0/1: Batch 702/782: Batch Loss: 0.6163656711578369\n",
            "Epoch 0/1: Batch 703/782: Batch Loss: 0.6985738277435303\n",
            "Epoch 0/1: Batch 704/782: Batch Loss: 0.5645855069160461\n",
            "Epoch 0/1: Batch 705/782: Batch Loss: 0.3496318459510803\n",
            "Epoch 0/1: Batch 706/782: Batch Loss: 0.49245485663414\n",
            "Epoch 0/1: Batch 707/782: Batch Loss: 0.6874947547912598\n",
            "Epoch 0/1: Batch 708/782: Batch Loss: 0.6303781270980835\n",
            "Epoch 0/1: Batch 709/782: Batch Loss: 0.6392483711242676\n",
            "Epoch 0/1: Batch 710/782: Batch Loss: 0.7347170114517212\n",
            "Epoch 0/1: Batch 711/782: Batch Loss: 0.7048786282539368\n",
            "Epoch 0/1: Batch 712/782: Batch Loss: 0.719391942024231\n",
            "Epoch 0/1: Batch 713/782: Batch Loss: 0.5753539800643921\n",
            "Epoch 0/1: Batch 714/782: Batch Loss: 0.6933419108390808\n",
            "Epoch 0/1: Batch 715/782: Batch Loss: 0.7794840931892395\n",
            "Epoch 0/1: Batch 716/782: Batch Loss: 0.5590147376060486\n",
            "Epoch 0/1: Batch 717/782: Batch Loss: 0.6380398869514465\n",
            "Epoch 0/1: Batch 718/782: Batch Loss: 0.35183799266815186\n",
            "Epoch 0/1: Batch 719/782: Batch Loss: 0.6476649045944214\n",
            "Epoch 0/1: Batch 720/782: Batch Loss: 0.5648764967918396\n",
            "Epoch 0/1: Batch 721/782: Batch Loss: 0.550030529499054\n",
            "Epoch 0/1: Batch 722/782: Batch Loss: 0.5555060505867004\n",
            "Epoch 0/1: Batch 723/782: Batch Loss: 0.6484518051147461\n",
            "Epoch 0/1: Batch 724/782: Batch Loss: 0.6923372745513916\n",
            "Epoch 0/1: Batch 725/782: Batch Loss: 0.6357949376106262\n",
            "Epoch 0/1: Batch 726/782: Batch Loss: 0.6770128607749939\n",
            "Epoch 0/1: Batch 727/782: Batch Loss: 0.6226987242698669\n",
            "Epoch 0/1: Batch 728/782: Batch Loss: 0.6589032411575317\n",
            "Epoch 0/1: Batch 729/782: Batch Loss: 0.9183182716369629\n",
            "Epoch 0/1: Batch 730/782: Batch Loss: 0.7769878506660461\n",
            "Epoch 0/1: Batch 731/782: Batch Loss: 0.7429360151290894\n",
            "Epoch 0/1: Batch 732/782: Batch Loss: 0.7926074862480164\n",
            "Epoch 0/1: Batch 733/782: Batch Loss: 0.6521517634391785\n",
            "Epoch 0/1: Batch 734/782: Batch Loss: 0.5196290016174316\n",
            "Epoch 0/1: Batch 735/782: Batch Loss: 0.5423856973648071\n",
            "Epoch 0/1: Batch 736/782: Batch Loss: 0.4609467387199402\n",
            "Epoch 0/1: Batch 737/782: Batch Loss: 0.5304123759269714\n",
            "Epoch 0/1: Batch 738/782: Batch Loss: 0.5238386392593384\n",
            "Epoch 0/1: Batch 739/782: Batch Loss: 0.6119328141212463\n",
            "Epoch 0/1: Batch 740/782: Batch Loss: 0.589061975479126\n",
            "Epoch 0/1: Batch 741/782: Batch Loss: 0.4981535077095032\n",
            "Epoch 0/1: Batch 742/782: Batch Loss: 0.6229232549667358\n",
            "Epoch 0/1: Batch 743/782: Batch Loss: 0.5699071884155273\n",
            "Epoch 0/1: Batch 744/782: Batch Loss: 0.5736103653907776\n",
            "Epoch 0/1: Batch 745/782: Batch Loss: 0.8629512786865234\n",
            "Epoch 0/1: Batch 746/782: Batch Loss: 0.6910088658332825\n",
            "Epoch 0/1: Batch 747/782: Batch Loss: 0.4712032377719879\n",
            "Epoch 0/1: Batch 748/782: Batch Loss: 0.6330666542053223\n",
            "Epoch 0/1: Batch 749/782: Batch Loss: 0.629996120929718\n",
            "Epoch 0/1: Batch 750/782: Batch Loss: 0.659972071647644\n",
            "Epoch 0/1: Batch 751/782: Batch Loss: 0.5866856575012207\n",
            "Epoch 0/1: Batch 752/782: Batch Loss: 0.8770875930786133\n",
            "Epoch 0/1: Batch 753/782: Batch Loss: 0.5703034996986389\n",
            "Epoch 0/1: Batch 754/782: Batch Loss: 0.6808770895004272\n",
            "Epoch 0/1: Batch 755/782: Batch Loss: 0.6205817461013794\n",
            "Epoch 0/1: Batch 756/782: Batch Loss: 0.46569469571113586\n",
            "Epoch 0/1: Batch 757/782: Batch Loss: 0.7303033471107483\n",
            "Epoch 0/1: Batch 758/782: Batch Loss: 0.5341061353683472\n",
            "Epoch 0/1: Batch 759/782: Batch Loss: 0.6534620523452759\n",
            "Epoch 0/1: Batch 760/782: Batch Loss: 0.621994137763977\n",
            "Epoch 0/1: Batch 761/782: Batch Loss: 0.5672126412391663\n",
            "Epoch 0/1: Batch 762/782: Batch Loss: 0.6426370143890381\n",
            "Epoch 0/1: Batch 763/782: Batch Loss: 0.5001428723335266\n",
            "Epoch 0/1: Batch 764/782: Batch Loss: 0.582567036151886\n",
            "Epoch 0/1: Batch 765/782: Batch Loss: 0.5391448140144348\n",
            "Epoch 0/1: Batch 766/782: Batch Loss: 0.562088131904602\n",
            "Epoch 0/1: Batch 767/782: Batch Loss: 0.5407159924507141\n",
            "Epoch 0/1: Batch 768/782: Batch Loss: 0.3923843502998352\n",
            "Epoch 0/1: Batch 769/782: Batch Loss: 0.5983152985572815\n",
            "Epoch 0/1: Batch 770/782: Batch Loss: 0.820189356803894\n",
            "Epoch 0/1: Batch 771/782: Batch Loss: 0.5770132541656494\n",
            "Epoch 0/1: Batch 772/782: Batch Loss: 0.526839017868042\n",
            "Epoch 0/1: Batch 773/782: Batch Loss: 0.61945641040802\n",
            "Epoch 0/1: Batch 774/782: Batch Loss: 0.6954200267791748\n",
            "Epoch 0/1: Batch 775/782: Batch Loss: 0.5097498893737793\n",
            "Epoch 0/1: Batch 776/782: Batch Loss: 0.3867403268814087\n",
            "Epoch 0/1: Batch 777/782: Batch Loss: 0.5721829533576965\n",
            "Epoch 0/1: Batch 778/782: Batch Loss: 0.6092814803123474\n",
            "Epoch 0/1: Batch 779/782: Batch Loss: 0.6284987926483154\n",
            "Epoch 0/1: Batch 780/782: Batch Loss: 0.6371549367904663\n",
            "Epoch 0/1: Batch 781/782: Batch Loss: 0.5060911178588867\n",
            "Epoch 0/1: Batch 782/782: Batch Loss: 0.4225914180278778\n",
            "Training Loss: 0.6957608210994765\n",
            "Batch 1/157: Accuracy: 89.0625 %\n",
            "Batch 2/157: Accuracy: 85.9375 %\n",
            "Batch 3/157: Accuracy: 78.125 %\n",
            "Batch 4/157: Accuracy: 70.3125 %\n",
            "Batch 5/157: Accuracy: 73.4375 %\n",
            "Batch 6/157: Accuracy: 82.8125 %\n",
            "Batch 7/157: Accuracy: 78.125 %\n",
            "Batch 8/157: Accuracy: 89.0625 %\n",
            "Batch 9/157: Accuracy: 84.375 %\n",
            "Batch 10/157: Accuracy: 79.6875 %\n",
            "Batch 11/157: Accuracy: 82.8125 %\n",
            "Batch 12/157: Accuracy: 85.9375 %\n",
            "Batch 13/157: Accuracy: 85.9375 %\n",
            "Batch 14/157: Accuracy: 78.125 %\n",
            "Batch 15/157: Accuracy: 81.25 %\n",
            "Batch 16/157: Accuracy: 79.6875 %\n",
            "Batch 17/157: Accuracy: 84.375 %\n",
            "Batch 18/157: Accuracy: 78.125 %\n",
            "Batch 19/157: Accuracy: 85.9375 %\n",
            "Batch 20/157: Accuracy: 79.6875 %\n",
            "Batch 21/157: Accuracy: 89.0625 %\n",
            "Batch 22/157: Accuracy: 79.6875 %\n",
            "Batch 23/157: Accuracy: 81.25 %\n",
            "Batch 24/157: Accuracy: 79.6875 %\n",
            "Batch 25/157: Accuracy: 78.125 %\n",
            "Batch 26/157: Accuracy: 82.8125 %\n",
            "Batch 27/157: Accuracy: 87.5 %\n",
            "Batch 28/157: Accuracy: 90.625 %\n",
            "Batch 29/157: Accuracy: 87.5 %\n",
            "Batch 30/157: Accuracy: 82.8125 %\n",
            "Batch 31/157: Accuracy: 70.3125 %\n",
            "Batch 32/157: Accuracy: 85.9375 %\n",
            "Batch 33/157: Accuracy: 84.375 %\n",
            "Batch 34/157: Accuracy: 75.0 %\n",
            "Batch 35/157: Accuracy: 84.375 %\n",
            "Batch 36/157: Accuracy: 85.9375 %\n",
            "Batch 37/157: Accuracy: 81.25 %\n",
            "Batch 38/157: Accuracy: 78.125 %\n",
            "Batch 39/157: Accuracy: 84.375 %\n",
            "Batch 40/157: Accuracy: 64.0625 %\n",
            "Batch 41/157: Accuracy: 79.6875 %\n",
            "Batch 42/157: Accuracy: 87.5 %\n",
            "Batch 43/157: Accuracy: 89.0625 %\n",
            "Batch 44/157: Accuracy: 79.6875 %\n",
            "Batch 45/157: Accuracy: 79.6875 %\n",
            "Batch 46/157: Accuracy: 82.8125 %\n",
            "Batch 47/157: Accuracy: 82.8125 %\n",
            "Batch 48/157: Accuracy: 81.25 %\n",
            "Batch 49/157: Accuracy: 82.8125 %\n",
            "Batch 50/157: Accuracy: 87.5 %\n",
            "Batch 51/157: Accuracy: 81.25 %\n",
            "Batch 52/157: Accuracy: 81.25 %\n",
            "Batch 53/157: Accuracy: 79.6875 %\n",
            "Batch 54/157: Accuracy: 81.25 %\n",
            "Batch 55/157: Accuracy: 87.5 %\n",
            "Batch 56/157: Accuracy: 84.375 %\n",
            "Batch 57/157: Accuracy: 81.25 %\n",
            "Batch 58/157: Accuracy: 84.375 %\n",
            "Batch 59/157: Accuracy: 75.0 %\n",
            "Batch 60/157: Accuracy: 82.8125 %\n",
            "Batch 61/157: Accuracy: 87.5 %\n",
            "Batch 62/157: Accuracy: 84.375 %\n",
            "Batch 63/157: Accuracy: 82.8125 %\n",
            "Batch 64/157: Accuracy: 84.375 %\n",
            "Batch 65/157: Accuracy: 82.8125 %\n",
            "Batch 66/157: Accuracy: 89.0625 %\n",
            "Batch 67/157: Accuracy: 79.6875 %\n",
            "Batch 68/157: Accuracy: 85.9375 %\n",
            "Batch 69/157: Accuracy: 87.5 %\n",
            "Batch 70/157: Accuracy: 87.5 %\n",
            "Batch 71/157: Accuracy: 85.9375 %\n",
            "Batch 72/157: Accuracy: 79.6875 %\n",
            "Batch 73/157: Accuracy: 87.5 %\n",
            "Batch 74/157: Accuracy: 81.25 %\n",
            "Batch 75/157: Accuracy: 79.6875 %\n",
            "Batch 76/157: Accuracy: 92.1875 %\n",
            "Batch 77/157: Accuracy: 82.8125 %\n",
            "Batch 78/157: Accuracy: 73.4375 %\n",
            "Batch 79/157: Accuracy: 79.6875 %\n",
            "Batch 80/157: Accuracy: 71.875 %\n",
            "Batch 81/157: Accuracy: 87.5 %\n",
            "Batch 82/157: Accuracy: 70.3125 %\n",
            "Batch 83/157: Accuracy: 82.8125 %\n",
            "Batch 84/157: Accuracy: 84.375 %\n",
            "Batch 85/157: Accuracy: 79.6875 %\n",
            "Batch 86/157: Accuracy: 78.125 %\n",
            "Batch 87/157: Accuracy: 75.0 %\n",
            "Batch 88/157: Accuracy: 82.8125 %\n",
            "Batch 89/157: Accuracy: 75.0 %\n",
            "Batch 90/157: Accuracy: 81.25 %\n",
            "Batch 91/157: Accuracy: 81.25 %\n",
            "Batch 92/157: Accuracy: 70.3125 %\n",
            "Batch 93/157: Accuracy: 81.25 %\n",
            "Batch 94/157: Accuracy: 84.375 %\n",
            "Batch 95/157: Accuracy: 79.6875 %\n",
            "Batch 96/157: Accuracy: 90.625 %\n",
            "Batch 97/157: Accuracy: 81.25 %\n",
            "Batch 98/157: Accuracy: 81.25 %\n",
            "Batch 99/157: Accuracy: 90.625 %\n",
            "Batch 100/157: Accuracy: 89.0625 %\n",
            "Batch 101/157: Accuracy: 84.375 %\n",
            "Batch 102/157: Accuracy: 81.25 %\n",
            "Batch 103/157: Accuracy: 73.4375 %\n",
            "Batch 104/157: Accuracy: 89.0625 %\n",
            "Batch 105/157: Accuracy: 84.375 %\n",
            "Batch 106/157: Accuracy: 84.375 %\n",
            "Batch 107/157: Accuracy: 85.9375 %\n",
            "Batch 108/157: Accuracy: 81.25 %\n",
            "Batch 109/157: Accuracy: 78.125 %\n",
            "Batch 110/157: Accuracy: 76.5625 %\n",
            "Batch 111/157: Accuracy: 76.5625 %\n",
            "Batch 112/157: Accuracy: 82.8125 %\n",
            "Batch 113/157: Accuracy: 81.25 %\n",
            "Batch 114/157: Accuracy: 87.5 %\n",
            "Batch 115/157: Accuracy: 87.5 %\n",
            "Batch 116/157: Accuracy: 84.375 %\n",
            "Batch 117/157: Accuracy: 85.9375 %\n",
            "Batch 118/157: Accuracy: 81.25 %\n",
            "Batch 119/157: Accuracy: 85.9375 %\n",
            "Batch 120/157: Accuracy: 84.375 %\n",
            "Batch 121/157: Accuracy: 82.8125 %\n",
            "Batch 122/157: Accuracy: 81.25 %\n",
            "Batch 123/157: Accuracy: 81.25 %\n",
            "Batch 124/157: Accuracy: 89.0625 %\n",
            "Batch 125/157: Accuracy: 78.125 %\n",
            "Batch 126/157: Accuracy: 84.375 %\n",
            "Batch 127/157: Accuracy: 82.8125 %\n",
            "Batch 128/157: Accuracy: 93.75 %\n",
            "Batch 129/157: Accuracy: 81.25 %\n",
            "Batch 130/157: Accuracy: 76.5625 %\n",
            "Batch 131/157: Accuracy: 82.8125 %\n",
            "Batch 132/157: Accuracy: 81.25 %\n",
            "Batch 133/157: Accuracy: 79.6875 %\n",
            "Batch 134/157: Accuracy: 78.125 %\n",
            "Batch 135/157: Accuracy: 82.8125 %\n",
            "Batch 136/157: Accuracy: 84.375 %\n",
            "Batch 137/157: Accuracy: 70.3125 %\n",
            "Batch 138/157: Accuracy: 70.3125 %\n",
            "Batch 139/157: Accuracy: 85.9375 %\n",
            "Batch 140/157: Accuracy: 84.375 %\n",
            "Batch 141/157: Accuracy: 84.375 %\n",
            "Batch 142/157: Accuracy: 84.375 %\n",
            "Batch 143/157: Accuracy: 75.0 %\n",
            "Batch 144/157: Accuracy: 85.9375 %\n",
            "Batch 145/157: Accuracy: 73.4375 %\n",
            "Batch 146/157: Accuracy: 84.375 %\n",
            "Batch 147/157: Accuracy: 79.6875 %\n",
            "Batch 148/157: Accuracy: 84.375 %\n",
            "Batch 149/157: Accuracy: 82.8125 %\n",
            "Batch 150/157: Accuracy: 89.0625 %\n",
            "Batch 151/157: Accuracy: 84.375 %\n",
            "Batch 152/157: Accuracy: 81.25 %\n",
            "Batch 153/157: Accuracy: 78.125 %\n",
            "Batch 154/157: Accuracy: 81.25 %\n",
            "Batch 155/157: Accuracy: 82.8125 %\n",
            "Batch 156/157: Accuracy: 87.5 %\n",
            "Batch 157/157: Accuracy: 75.0 %\n",
            "Total Accuracy after 10000 images: 82.14 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UK5cq8wm0yz",
        "colab_type": "code",
        "outputId": "319fae26-c3de-48a5-de9e-3378e076f3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=512, out_features=10, bias=True)\n",
              "      (4): LogSoftmax()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17pPsqSGsNWX",
        "colab_type": "code",
        "outputId": "9bf996a9-c07a-40ba-fd40-7b0b31898332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=512, out_features=10, bias=True)\n",
              "      (4): LogSoftmax()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNkmUgKdqnzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to('cpu')\n",
        "for i in range(0,6):\n",
        "  model.classifier[i].requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXoLZiu8qWNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for params in model.features.parameters():\n",
        "  params.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8USmYvVpsfNE",
        "colab_type": "code",
        "outputId": "aa6afa8f-88e6-4650-be04-33b6b529e88e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "for i in range(0,31):\n",
        "  model.features[i].requires_grad = True\n",
        "for params in model.features.parameters():\n",
        "  print(params.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6NGH1SBts8s",
        "colab_type": "code",
        "outputId": "7aaf5d70-5b8a-4024-91f8-a34ae1b183b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t classifier.6.0.weight\n",
            "\t classifier.6.0.bias\n",
            "\t classifier.6.3.weight\n",
            "\t classifier.6.3.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHFROfjvzGrl",
        "colab_type": "code",
        "outputId": "e0aec5ca-e8f3-477c-9f47-a7eb7e436eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'{total_params:,} total parameters.')\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{total_trainable_params:,} training parameters.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136,363,338 total parameters.\n",
            "2,102,794 training parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meBVolw4xHgf",
        "colab_type": "code",
        "outputId": "54f231bc-e06e-4c42-f979-ff137d9914d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "a09d9dfdcb554aa5a9870c6770449267",
            "17b57f58ca9c44c7a229ec43d0d811ad",
            "4edb3d442d6d4d43935db0ee8e6bc913",
            "d7203fd5059248e8bf757231169c3aaa",
            "9a898d28c0a547f48630e933565153cf",
            "156845c1c9254b70a9b078219395c19a",
            "8efad0ce99a94284a3bc6091bf2ae4ff",
            "946c0674a536456c92f7da15362a9b92"
          ]
        }
      },
      "source": [
        "res50_model = models.resnet50(pretrained=True)\n",
        "res50_conv = nn.Sequential(*list(res50_model.children())[:-2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a09d9dfdcb554aa5a9870c6770449267",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=102502400), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OucOPOQLxX-Y",
        "colab_type": "code",
        "outputId": "08a41725-2522-4a92-baf0-885e6faf697c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nn.Sequential(*list(res50_model.children())[:-2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLT_dPeixOPK",
        "colab_type": "code",
        "outputId": "d2b3ad92-1be8-42d6-ebb9-228b1796c043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for name,param in res50_conv.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t 0.weight\n",
            "\t 1.weight\n",
            "\t 1.bias\n",
            "\t 4.0.conv1.weight\n",
            "\t 4.0.bn1.weight\n",
            "\t 4.0.bn1.bias\n",
            "\t 4.0.conv2.weight\n",
            "\t 4.0.bn2.weight\n",
            "\t 4.0.bn2.bias\n",
            "\t 4.0.conv3.weight\n",
            "\t 4.0.bn3.weight\n",
            "\t 4.0.bn3.bias\n",
            "\t 4.0.downsample.0.weight\n",
            "\t 4.0.downsample.1.weight\n",
            "\t 4.0.downsample.1.bias\n",
            "\t 4.1.conv1.weight\n",
            "\t 4.1.bn1.weight\n",
            "\t 4.1.bn1.bias\n",
            "\t 4.1.conv2.weight\n",
            "\t 4.1.bn2.weight\n",
            "\t 4.1.bn2.bias\n",
            "\t 4.1.conv3.weight\n",
            "\t 4.1.bn3.weight\n",
            "\t 4.1.bn3.bias\n",
            "\t 4.2.conv1.weight\n",
            "\t 4.2.bn1.weight\n",
            "\t 4.2.bn1.bias\n",
            "\t 4.2.conv2.weight\n",
            "\t 4.2.bn2.weight\n",
            "\t 4.2.bn2.bias\n",
            "\t 4.2.conv3.weight\n",
            "\t 4.2.bn3.weight\n",
            "\t 4.2.bn3.bias\n",
            "\t 5.0.conv1.weight\n",
            "\t 5.0.bn1.weight\n",
            "\t 5.0.bn1.bias\n",
            "\t 5.0.conv2.weight\n",
            "\t 5.0.bn2.weight\n",
            "\t 5.0.bn2.bias\n",
            "\t 5.0.conv3.weight\n",
            "\t 5.0.bn3.weight\n",
            "\t 5.0.bn3.bias\n",
            "\t 5.0.downsample.0.weight\n",
            "\t 5.0.downsample.1.weight\n",
            "\t 5.0.downsample.1.bias\n",
            "\t 5.1.conv1.weight\n",
            "\t 5.1.bn1.weight\n",
            "\t 5.1.bn1.bias\n",
            "\t 5.1.conv2.weight\n",
            "\t 5.1.bn2.weight\n",
            "\t 5.1.bn2.bias\n",
            "\t 5.1.conv3.weight\n",
            "\t 5.1.bn3.weight\n",
            "\t 5.1.bn3.bias\n",
            "\t 5.2.conv1.weight\n",
            "\t 5.2.bn1.weight\n",
            "\t 5.2.bn1.bias\n",
            "\t 5.2.conv2.weight\n",
            "\t 5.2.bn2.weight\n",
            "\t 5.2.bn2.bias\n",
            "\t 5.2.conv3.weight\n",
            "\t 5.2.bn3.weight\n",
            "\t 5.2.bn3.bias\n",
            "\t 5.3.conv1.weight\n",
            "\t 5.3.bn1.weight\n",
            "\t 5.3.bn1.bias\n",
            "\t 5.3.conv2.weight\n",
            "\t 5.3.bn2.weight\n",
            "\t 5.3.bn2.bias\n",
            "\t 5.3.conv3.weight\n",
            "\t 5.3.bn3.weight\n",
            "\t 5.3.bn3.bias\n",
            "\t 6.0.conv1.weight\n",
            "\t 6.0.bn1.weight\n",
            "\t 6.0.bn1.bias\n",
            "\t 6.0.conv2.weight\n",
            "\t 6.0.bn2.weight\n",
            "\t 6.0.bn2.bias\n",
            "\t 6.0.conv3.weight\n",
            "\t 6.0.bn3.weight\n",
            "\t 6.0.bn3.bias\n",
            "\t 6.0.downsample.0.weight\n",
            "\t 6.0.downsample.1.weight\n",
            "\t 6.0.downsample.1.bias\n",
            "\t 6.1.conv1.weight\n",
            "\t 6.1.bn1.weight\n",
            "\t 6.1.bn1.bias\n",
            "\t 6.1.conv2.weight\n",
            "\t 6.1.bn2.weight\n",
            "\t 6.1.bn2.bias\n",
            "\t 6.1.conv3.weight\n",
            "\t 6.1.bn3.weight\n",
            "\t 6.1.bn3.bias\n",
            "\t 6.2.conv1.weight\n",
            "\t 6.2.bn1.weight\n",
            "\t 6.2.bn1.bias\n",
            "\t 6.2.conv2.weight\n",
            "\t 6.2.bn2.weight\n",
            "\t 6.2.bn2.bias\n",
            "\t 6.2.conv3.weight\n",
            "\t 6.2.bn3.weight\n",
            "\t 6.2.bn3.bias\n",
            "\t 6.3.conv1.weight\n",
            "\t 6.3.bn1.weight\n",
            "\t 6.3.bn1.bias\n",
            "\t 6.3.conv2.weight\n",
            "\t 6.3.bn2.weight\n",
            "\t 6.3.bn2.bias\n",
            "\t 6.3.conv3.weight\n",
            "\t 6.3.bn3.weight\n",
            "\t 6.3.bn3.bias\n",
            "\t 6.4.conv1.weight\n",
            "\t 6.4.bn1.weight\n",
            "\t 6.4.bn1.bias\n",
            "\t 6.4.conv2.weight\n",
            "\t 6.4.bn2.weight\n",
            "\t 6.4.bn2.bias\n",
            "\t 6.4.conv3.weight\n",
            "\t 6.4.bn3.weight\n",
            "\t 6.4.bn3.bias\n",
            "\t 6.5.conv1.weight\n",
            "\t 6.5.bn1.weight\n",
            "\t 6.5.bn1.bias\n",
            "\t 6.5.conv2.weight\n",
            "\t 6.5.bn2.weight\n",
            "\t 6.5.bn2.bias\n",
            "\t 6.5.conv3.weight\n",
            "\t 6.5.bn3.weight\n",
            "\t 6.5.bn3.bias\n",
            "\t 7.0.conv1.weight\n",
            "\t 7.0.bn1.weight\n",
            "\t 7.0.bn1.bias\n",
            "\t 7.0.conv2.weight\n",
            "\t 7.0.bn2.weight\n",
            "\t 7.0.bn2.bias\n",
            "\t 7.0.conv3.weight\n",
            "\t 7.0.bn3.weight\n",
            "\t 7.0.bn3.bias\n",
            "\t 7.0.downsample.0.weight\n",
            "\t 7.0.downsample.1.weight\n",
            "\t 7.0.downsample.1.bias\n",
            "\t 7.1.conv1.weight\n",
            "\t 7.1.bn1.weight\n",
            "\t 7.1.bn1.bias\n",
            "\t 7.1.conv2.weight\n",
            "\t 7.1.bn2.weight\n",
            "\t 7.1.bn2.bias\n",
            "\t 7.1.conv3.weight\n",
            "\t 7.1.bn3.weight\n",
            "\t 7.1.bn3.bias\n",
            "\t 7.2.conv1.weight\n",
            "\t 7.2.bn1.weight\n",
            "\t 7.2.bn1.bias\n",
            "\t 7.2.conv2.weight\n",
            "\t 7.2.bn2.weight\n",
            "\t 7.2.bn2.bias\n",
            "\t 7.2.conv3.weight\n",
            "\t 7.2.bn3.weight\n",
            "\t 7.2.bn3.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmHxgYIMxMGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in res50_conv.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMgby0VIpVN2",
        "colab_type": "code",
        "outputId": "d78191f1-0074-44f9-cd06-43a2cef64c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.train()\n",
        "model.to(device)\n",
        "num_epochs = 1\n",
        "batch_loss = 0\n",
        "cum_loss = 0\n",
        "for e in range(num_epochs):\n",
        "  for batch, (images,labels) in enumerate(trainloader,1):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logps=model(images)\n",
        "    loss=criterion(logps,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_loss += loss.item()\n",
        "    print(f'Epoch {e}/{num_epochs}: Batch {batch}/{len(trainloader)}: Batch Loss: {loss.item()}')\n",
        "print(f'Training Loss: {batch_loss/len(trainloader)}')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "num_correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for batch, (images, labels) in enumerate(testloader,1):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    logps=model(images)\n",
        "    output = torch.exp(logps)\n",
        "    pred = torch.argmax(output,1)\n",
        "    total += labels.size(0)\n",
        "    num_correct += (pred == labels).sum().item()\n",
        "    print(f'Batch {batch}/{len(testloader)}: Accuracy: {(pred == labels).sum().item()*100/labels.size(0)} %') \n",
        "  print(f'Total Accuracy after {total} images: {num_correct*100/total} %')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Batch 1/782: Batch Loss: 0.7414603233337402\n",
            "Epoch 0/1: Batch 2/782: Batch Loss: 0.3884875178337097\n",
            "Epoch 0/1: Batch 3/782: Batch Loss: 0.8433461785316467\n",
            "Epoch 0/1: Batch 4/782: Batch Loss: 0.5378896594047546\n",
            "Epoch 0/1: Batch 5/782: Batch Loss: 0.42450791597366333\n",
            "Epoch 0/1: Batch 6/782: Batch Loss: 0.6997003555297852\n",
            "Epoch 0/1: Batch 7/782: Batch Loss: 0.5680793523788452\n",
            "Epoch 0/1: Batch 8/782: Batch Loss: 0.4067124128341675\n",
            "Epoch 0/1: Batch 9/782: Batch Loss: 0.4366336762905121\n",
            "Epoch 0/1: Batch 10/782: Batch Loss: 0.5363320112228394\n",
            "Epoch 0/1: Batch 11/782: Batch Loss: 0.4877561330795288\n",
            "Epoch 0/1: Batch 12/782: Batch Loss: 0.7921039462089539\n",
            "Epoch 0/1: Batch 13/782: Batch Loss: 0.31972748041152954\n",
            "Epoch 0/1: Batch 14/782: Batch Loss: 0.5147030353546143\n",
            "Epoch 0/1: Batch 15/782: Batch Loss: 0.7436081171035767\n",
            "Epoch 0/1: Batch 16/782: Batch Loss: 0.6659941673278809\n",
            "Epoch 0/1: Batch 17/782: Batch Loss: 0.6033780574798584\n",
            "Epoch 0/1: Batch 18/782: Batch Loss: 0.725317656993866\n",
            "Epoch 0/1: Batch 19/782: Batch Loss: 0.46015504002571106\n",
            "Epoch 0/1: Batch 20/782: Batch Loss: 0.5243472456932068\n",
            "Epoch 0/1: Batch 21/782: Batch Loss: 0.5003712773323059\n",
            "Epoch 0/1: Batch 22/782: Batch Loss: 0.6358771324157715\n",
            "Epoch 0/1: Batch 23/782: Batch Loss: 0.7541519999504089\n",
            "Epoch 0/1: Batch 24/782: Batch Loss: 0.4080507159233093\n",
            "Epoch 0/1: Batch 25/782: Batch Loss: 0.7063962817192078\n",
            "Epoch 0/1: Batch 26/782: Batch Loss: 0.6605396270751953\n",
            "Epoch 0/1: Batch 27/782: Batch Loss: 0.4361518621444702\n",
            "Epoch 0/1: Batch 28/782: Batch Loss: 0.725267767906189\n",
            "Epoch 0/1: Batch 29/782: Batch Loss: 0.7087839245796204\n",
            "Epoch 0/1: Batch 30/782: Batch Loss: 0.6130528450012207\n",
            "Epoch 0/1: Batch 31/782: Batch Loss: 0.5808880925178528\n",
            "Epoch 0/1: Batch 32/782: Batch Loss: 0.6726804971694946\n",
            "Epoch 0/1: Batch 33/782: Batch Loss: 0.6518593430519104\n",
            "Epoch 0/1: Batch 34/782: Batch Loss: 0.516289472579956\n",
            "Epoch 0/1: Batch 35/782: Batch Loss: 0.3258149027824402\n",
            "Epoch 0/1: Batch 36/782: Batch Loss: 0.49883633852005005\n",
            "Epoch 0/1: Batch 37/782: Batch Loss: 0.5980254411697388\n",
            "Epoch 0/1: Batch 38/782: Batch Loss: 0.6797106266021729\n",
            "Epoch 0/1: Batch 39/782: Batch Loss: 0.47427722811698914\n",
            "Epoch 0/1: Batch 40/782: Batch Loss: 0.7187853455543518\n",
            "Epoch 0/1: Batch 41/782: Batch Loss: 0.7053020000457764\n",
            "Epoch 0/1: Batch 42/782: Batch Loss: 0.604279637336731\n",
            "Epoch 0/1: Batch 43/782: Batch Loss: 0.5495006442070007\n",
            "Epoch 0/1: Batch 44/782: Batch Loss: 0.571025550365448\n",
            "Epoch 0/1: Batch 45/782: Batch Loss: 0.5006343722343445\n",
            "Epoch 0/1: Batch 46/782: Batch Loss: 0.4854090213775635\n",
            "Epoch 0/1: Batch 47/782: Batch Loss: 0.5115542411804199\n",
            "Epoch 0/1: Batch 48/782: Batch Loss: 0.5331507921218872\n",
            "Epoch 0/1: Batch 49/782: Batch Loss: 0.5797902345657349\n",
            "Epoch 0/1: Batch 50/782: Batch Loss: 0.6180377006530762\n",
            "Epoch 0/1: Batch 51/782: Batch Loss: 0.6622568368911743\n",
            "Epoch 0/1: Batch 52/782: Batch Loss: 0.36629924178123474\n",
            "Epoch 0/1: Batch 53/782: Batch Loss: 0.6064802408218384\n",
            "Epoch 0/1: Batch 54/782: Batch Loss: 0.4929725229740143\n",
            "Epoch 0/1: Batch 55/782: Batch Loss: 0.7392067313194275\n",
            "Epoch 0/1: Batch 56/782: Batch Loss: 0.8312780857086182\n",
            "Epoch 0/1: Batch 57/782: Batch Loss: 0.5771597623825073\n",
            "Epoch 0/1: Batch 58/782: Batch Loss: 0.5644327402114868\n",
            "Epoch 0/1: Batch 59/782: Batch Loss: 0.7564137578010559\n",
            "Epoch 0/1: Batch 60/782: Batch Loss: 0.7142425775527954\n",
            "Epoch 0/1: Batch 61/782: Batch Loss: 0.6082563400268555\n",
            "Epoch 0/1: Batch 62/782: Batch Loss: 0.747283399105072\n",
            "Epoch 0/1: Batch 63/782: Batch Loss: 0.54189133644104\n",
            "Epoch 0/1: Batch 64/782: Batch Loss: 0.6222841739654541\n",
            "Epoch 0/1: Batch 65/782: Batch Loss: 0.6999470591545105\n",
            "Epoch 0/1: Batch 66/782: Batch Loss: 0.6828951835632324\n",
            "Epoch 0/1: Batch 67/782: Batch Loss: 0.6149215698242188\n",
            "Epoch 0/1: Batch 68/782: Batch Loss: 0.42282700538635254\n",
            "Epoch 0/1: Batch 69/782: Batch Loss: 0.6586676836013794\n",
            "Epoch 0/1: Batch 70/782: Batch Loss: 0.444284051656723\n",
            "Epoch 0/1: Batch 71/782: Batch Loss: 0.6087418794631958\n",
            "Epoch 0/1: Batch 72/782: Batch Loss: 0.7376911640167236\n",
            "Epoch 0/1: Batch 73/782: Batch Loss: 0.4837714433670044\n",
            "Epoch 0/1: Batch 74/782: Batch Loss: 0.6712889075279236\n",
            "Epoch 0/1: Batch 75/782: Batch Loss: 0.46579474210739136\n",
            "Epoch 0/1: Batch 76/782: Batch Loss: 0.5218687057495117\n",
            "Epoch 0/1: Batch 77/782: Batch Loss: 0.6947085857391357\n",
            "Epoch 0/1: Batch 78/782: Batch Loss: 0.555472731590271\n",
            "Epoch 0/1: Batch 79/782: Batch Loss: 0.6672446727752686\n",
            "Epoch 0/1: Batch 80/782: Batch Loss: 0.547191321849823\n",
            "Epoch 0/1: Batch 81/782: Batch Loss: 0.5580465793609619\n",
            "Epoch 0/1: Batch 82/782: Batch Loss: 0.5133252739906311\n",
            "Epoch 0/1: Batch 83/782: Batch Loss: 0.5903462767601013\n",
            "Epoch 0/1: Batch 84/782: Batch Loss: 0.4721757471561432\n",
            "Epoch 0/1: Batch 85/782: Batch Loss: 0.48349717259407043\n",
            "Epoch 0/1: Batch 86/782: Batch Loss: 0.6511973142623901\n",
            "Epoch 0/1: Batch 87/782: Batch Loss: 0.4296528398990631\n",
            "Epoch 0/1: Batch 88/782: Batch Loss: 0.6458935737609863\n",
            "Epoch 0/1: Batch 89/782: Batch Loss: 0.5245682001113892\n",
            "Epoch 0/1: Batch 90/782: Batch Loss: 0.47284942865371704\n",
            "Epoch 0/1: Batch 91/782: Batch Loss: 0.6141848564147949\n",
            "Epoch 0/1: Batch 92/782: Batch Loss: 0.5924383401870728\n",
            "Epoch 0/1: Batch 93/782: Batch Loss: 0.674384355545044\n",
            "Epoch 0/1: Batch 94/782: Batch Loss: 0.5150564312934875\n",
            "Epoch 0/1: Batch 95/782: Batch Loss: 0.49287673830986023\n",
            "Epoch 0/1: Batch 96/782: Batch Loss: 0.6490477323532104\n",
            "Epoch 0/1: Batch 97/782: Batch Loss: 0.47864317893981934\n",
            "Epoch 0/1: Batch 98/782: Batch Loss: 0.46871453523635864\n",
            "Epoch 0/1: Batch 99/782: Batch Loss: 0.7004601955413818\n",
            "Epoch 0/1: Batch 100/782: Batch Loss: 0.5248020887374878\n",
            "Epoch 0/1: Batch 101/782: Batch Loss: 0.4582289457321167\n",
            "Epoch 0/1: Batch 102/782: Batch Loss: 0.438374400138855\n",
            "Epoch 0/1: Batch 103/782: Batch Loss: 0.4802631735801697\n",
            "Epoch 0/1: Batch 104/782: Batch Loss: 0.6941139698028564\n",
            "Epoch 0/1: Batch 105/782: Batch Loss: 0.7216817736625671\n",
            "Epoch 0/1: Batch 106/782: Batch Loss: 0.7885332703590393\n",
            "Epoch 0/1: Batch 107/782: Batch Loss: 0.6811118125915527\n",
            "Epoch 0/1: Batch 108/782: Batch Loss: 0.755836009979248\n",
            "Epoch 0/1: Batch 109/782: Batch Loss: 0.6937490105628967\n",
            "Epoch 0/1: Batch 110/782: Batch Loss: 0.5640612244606018\n",
            "Epoch 0/1: Batch 111/782: Batch Loss: 0.41658055782318115\n",
            "Epoch 0/1: Batch 112/782: Batch Loss: 0.33104321360588074\n",
            "Epoch 0/1: Batch 113/782: Batch Loss: 0.5297231674194336\n",
            "Epoch 0/1: Batch 114/782: Batch Loss: 0.5961329340934753\n",
            "Epoch 0/1: Batch 115/782: Batch Loss: 0.5821870565414429\n",
            "Epoch 0/1: Batch 116/782: Batch Loss: 0.6085128784179688\n",
            "Epoch 0/1: Batch 117/782: Batch Loss: 0.5168859362602234\n",
            "Epoch 0/1: Batch 118/782: Batch Loss: 0.7896759510040283\n",
            "Epoch 0/1: Batch 119/782: Batch Loss: 0.48711806535720825\n",
            "Epoch 0/1: Batch 120/782: Batch Loss: 0.6254059672355652\n",
            "Epoch 0/1: Batch 121/782: Batch Loss: 0.46134626865386963\n",
            "Epoch 0/1: Batch 122/782: Batch Loss: 0.6304851174354553\n",
            "Epoch 0/1: Batch 123/782: Batch Loss: 0.5253256559371948\n",
            "Epoch 0/1: Batch 124/782: Batch Loss: 0.596989095211029\n",
            "Epoch 0/1: Batch 125/782: Batch Loss: 0.6428053379058838\n",
            "Epoch 0/1: Batch 126/782: Batch Loss: 0.5296299457550049\n",
            "Epoch 0/1: Batch 127/782: Batch Loss: 0.42727068066596985\n",
            "Epoch 0/1: Batch 128/782: Batch Loss: 0.5928040742874146\n",
            "Epoch 0/1: Batch 129/782: Batch Loss: 0.6985294222831726\n",
            "Epoch 0/1: Batch 130/782: Batch Loss: 0.7020511031150818\n",
            "Epoch 0/1: Batch 131/782: Batch Loss: 0.4025200307369232\n",
            "Epoch 0/1: Batch 132/782: Batch Loss: 0.7889530062675476\n",
            "Epoch 0/1: Batch 133/782: Batch Loss: 0.7321644425392151\n",
            "Epoch 0/1: Batch 134/782: Batch Loss: 0.612616777420044\n",
            "Epoch 0/1: Batch 135/782: Batch Loss: 0.8270431756973267\n",
            "Epoch 0/1: Batch 136/782: Batch Loss: 0.6369628310203552\n",
            "Epoch 0/1: Batch 137/782: Batch Loss: 0.6921634078025818\n",
            "Epoch 0/1: Batch 138/782: Batch Loss: 0.5304296612739563\n",
            "Epoch 0/1: Batch 139/782: Batch Loss: 0.6245275735855103\n",
            "Epoch 0/1: Batch 140/782: Batch Loss: 0.4307084381580353\n",
            "Epoch 0/1: Batch 141/782: Batch Loss: 0.627466082572937\n",
            "Epoch 0/1: Batch 142/782: Batch Loss: 0.6322606205940247\n",
            "Epoch 0/1: Batch 143/782: Batch Loss: 0.6613968014717102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-eb947cc5bedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {e}/{num_epochs}: Batch {batch}/{len(trainloader)}: Batch Loss: {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training Loss: {batch_loss/len(trainloader)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnwdP24zp_3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}